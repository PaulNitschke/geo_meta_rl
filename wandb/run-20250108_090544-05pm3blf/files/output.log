2025-01-08 09:05:46 | [CL_point_env] Logging to /Users/paulnitschke/Desktop/projects/geo_meta_rl/data/local/experiment/CL_point_env_24


Failed to import TF-Keras. Please note that TF-Keras is not installed by default when you install TensorFlow Probability. This is so that JAX-only users do not have to install TensorFlow or TF-Keras. To use TensorFlow Probability with TensorFlow, please install the tf-keras or tf-keras-nightly package.
This can be be done through installing the tensorflow-probability[tf] extra.
/Users/paulnitschke/Desktop/projects/geo_meta_rl/garage/experiment/deterministic.py:36: UserWarning: Enabeling deterministic mode in PyTorch can have a performance impact when using GPU.
  warnings.warn(


2025-01-08 09:05:46 | [CL_point_env] Obtaining samples...
/Users/paulnitschke/miniconda3/envs/env_geo_meta_rl/lib/python3.9/site-packages/torch/distributions/distribution.py:53: UserWarning: <class 'garage.torch.distributions.tanh_normal.TanhNormal'> does not define `arg_constraints`. Please set `arg_constraints = {}` or initialize the distribution with `validate_args=False` to turn off validation.
  warnings.warn(
/Users/paulnitschke/Desktop/projects/geo_meta_rl/garage/_dtypes.py:1051: UserWarning: Observation array([0., 0., 2.]) is outside observation_space Box(-inf, inf, (3,), float32)
  warnings.warn(
2025-01-08 09:05:49 | [CL_point_env] epoch #0 | Pre-Training Encoder...
2025-01-08 09:05:49 | [CL_point_env] epoch #0 | Pairwise Similarity Matrix: tensor([[1.0000, 0.9984],
        [0.9984, 1.0000]], grad_fn=<MmBackward0>)
2025-01-08 09:06:02 | [CL_point_env] epoch #0 | Pairwise Similarity Matrix: tensor([[ 1.0000, -0.9989],
        [-0.9989,  1.0000]], grad_fn=<MmBackward0>)
2025-01-08 09:06:02 | [CL_point_env] epoch #0 | Pre-Training Encoder Done...
2025-01-08 09:06:03 | [CL_point_env] epoch #0 | Pairwise Similarity Matrix: tensor([[ 1.0000, -0.9992],
        [-0.9992,  1.0000]], grad_fn=<MmBackward0>)
2025-01-08 09:06:03 | [CL_point_env] epoch #0 | Mean Task Embeddings (n_tasks x embedding_dim): tensor([[-0.2129, -0.6398, -0.0234, -0.2363,  0.6992],
        [ 0.2439,  0.6506,  0.0098,  0.2258, -0.6826]],
       grad_fn=<PermuteBackward0>)

2025-01-08 09:06:03 | [CL_point_env] epoch #0 | Training...
2025-01-08 09:06:03 | [CL_point_env] epoch #0 | Pairwise Similarity Matrix: tensor([[ 1.0000, -0.9995],
        [-0.9995,  1.0000]], grad_fn=<MmBackward0>)
2025-01-08 09:06:03 | [CL_point_env] epoch #0 | Mean Task Embeddings (n_tasks x embedding_dim): tensor([[-0.2176, -0.6409, -0.0176, -0.2324,  0.6982],
        [ 0.2298,  0.6439,  0.0367,  0.2496, -0.6843]],
       grad_fn=<PermuteBackward0>)

2025-01-08 09:06:03 | [CL_point_env] epoch #0 | Training Encoder

-----------------------------  -----------
Embedding/MeanContrastiveLoss    0.0579126
PolicyTraining/MeanQ1Vals      -22.432
PolicyTraining/MeanQ2Vals      -22.432
PolicyTraining/MeanVVals        -4.44753
PolicyTraining/PolicyLoss       21.2361
PolicyTraining/QfLoss           13.0887
PolicyTraining/VfLoss          294.253
-----------------------------  -----------
-----------------------------  -----------
Embedding/MeanContrastiveLoss    0.0579126
PolicyTraining/MeanQ1Vals      -22.432
PolicyTraining/MeanQ2Vals      -22.432
PolicyTraining/MeanVVals        -4.44753
PolicyTraining/PolicyLoss       21.2361
PolicyTraining/QfLoss           13.0887
PolicyTraining/VfLoss          294.253
-----------------------------  -----------
2025-01-08 09:06:24 | [CL_point_env] epoch #0 | Pairwise Similarity Matrix: tensor([[ 1.0000, -0.9996],
        [-0.9996,  1.0000]], grad_fn=<MmBackward0>)
2025-01-08 09:06:24 | [CL_point_env] epoch #0 | Mean Task Embeddings (n_tasks x embedding_dim): tensor([[-0.2265, -0.6436, -0.0178, -0.2303,  0.6936],
        [ 0.2053,  0.6374,  0.0311,  0.2396, -0.7022]],
       grad_fn=<PermuteBackward0>)
-----------------------------  -----------
Embedding/MeanContrastiveLoss    0.0579126
PolicyTraining/MeanQ1Vals      -22.432
PolicyTraining/MeanQ2Vals      -22.432
PolicyTraining/MeanVVals        -4.44753
PolicyTraining/PolicyLoss       21.2361
PolicyTraining/QfLoss           13.0887
PolicyTraining/VfLoss          294.253
-----------------------------  -----------
2025-01-08 09:06:24 | [CL_point_env] epoch #0 | Training Encoder

-----------------------------  -----------
Embedding/MeanContrastiveLoss    0.0578983
PolicyTraining/MeanQ1Vals      -22.4682
PolicyTraining/MeanQ2Vals      -22.4682
PolicyTraining/MeanVVals       -20.0202
PolicyTraining/PolicyLoss       21.225
PolicyTraining/QfLoss            5.27026
PolicyTraining/VfLoss            2.12084
-----------------------------  -----------
-----------------------------  -----------
Embedding/MeanContrastiveLoss    0.0578983
PolicyTraining/MeanQ1Vals      -22.4682
PolicyTraining/MeanQ2Vals      -22.4682
PolicyTraining/MeanVVals       -20.0202
PolicyTraining/PolicyLoss       21.225
PolicyTraining/QfLoss            5.27026
PolicyTraining/VfLoss            2.12084
-----------------------------  -----------
2025-01-08 09:06:46 | [CL_point_env] epoch #0 | Pairwise Similarity Matrix: tensor([[ 1.0000, -0.9997],
        [-0.9997,  1.0000]], grad_fn=<MmBackward0>)
2025-01-08 09:06:46 | [CL_point_env] epoch #0 | Mean Task Embeddings (n_tasks x embedding_dim): tensor([[-0.2275, -0.6439, -0.0222, -0.2335,  0.6919],
        [ 0.2084,  0.6405,  0.0156,  0.2241, -0.7042]],
       grad_fn=<PermuteBackward0>)
-----------------------------  -----------
Embedding/MeanContrastiveLoss    0.0578983
PolicyTraining/MeanQ1Vals      -22.4682
PolicyTraining/MeanQ2Vals      -22.4682
PolicyTraining/MeanVVals       -20.0202
PolicyTraining/PolicyLoss       21.225
PolicyTraining/QfLoss            5.27026
PolicyTraining/VfLoss            2.12084
-----------------------------  -----------
2025-01-08 09:06:46 | [CL_point_env] epoch #0 | Training Encoder

-----------------------------  -----------
Embedding/MeanContrastiveLoss    0.0579034
PolicyTraining/MeanQ1Vals      -24.3559
PolicyTraining/MeanQ2Vals      -24.3559
PolicyTraining/MeanVVals       -23.0708
PolicyTraining/PolicyLoss       23.1091
PolicyTraining/QfLoss            1.45557
PolicyTraining/VfLoss            0.999558
-----------------------------  -----------
-----------------------------  -----------
Embedding/MeanContrastiveLoss    0.0579034
PolicyTraining/MeanQ1Vals      -24.3559
PolicyTraining/MeanQ2Vals      -24.3559
PolicyTraining/MeanVVals       -23.0708
PolicyTraining/PolicyLoss       23.1091
PolicyTraining/QfLoss            1.45557
PolicyTraining/VfLoss            0.999558
-----------------------------  -----------
2025-01-08 09:07:07 | [CL_point_env] epoch #0 | Pairwise Similarity Matrix: tensor([[ 1.0000, -0.9999],
        [-0.9999,  1.0000]], grad_fn=<MmBackward0>)
2025-01-08 09:07:07 | [CL_point_env] epoch #0 | Mean Task Embeddings (n_tasks x embedding_dim): tensor([[-0.2206, -0.6424, -0.0231, -0.2358,  0.6946],
        [ 0.2232,  0.6426,  0.0152,  0.2290, -0.6960]],
       grad_fn=<PermuteBackward0>)
-----------------------------  -----------
Embedding/MeanContrastiveLoss    0.0579034
PolicyTraining/MeanQ1Vals      -24.3559
PolicyTraining/MeanQ2Vals      -24.3559
PolicyTraining/MeanVVals       -23.0708
PolicyTraining/PolicyLoss       23.1091
PolicyTraining/QfLoss            1.45557
PolicyTraining/VfLoss            0.999558
-----------------------------  -----------
2025-01-08 09:07:07 | [CL_point_env] epoch #0 | Training Encoder

-----------------------------  -----------
Embedding/MeanContrastiveLoss    0.0578935
PolicyTraining/MeanQ1Vals      -26.7977
PolicyTraining/MeanQ2Vals      -26.7977
PolicyTraining/MeanVVals       -25.6567
PolicyTraining/PolicyLoss       25.6114
PolicyTraining/QfLoss            0.485083
PolicyTraining/VfLoss            0.89481
-----------------------------  -----------
-----------------------------  -----------
Embedding/MeanContrastiveLoss    0.0578935
PolicyTraining/MeanQ1Vals      -26.7977
PolicyTraining/MeanQ2Vals      -26.7977
PolicyTraining/MeanVVals       -25.6567
PolicyTraining/PolicyLoss       25.6114
PolicyTraining/QfLoss            0.485083
PolicyTraining/VfLoss            0.89481
-----------------------------  -----------
2025-01-08 09:07:27 | [CL_point_env] epoch #0 | Pairwise Similarity Matrix: tensor([[ 1.0000, -0.9998],
        [-0.9998,  1.0000]], grad_fn=<MmBackward0>)
2025-01-08 09:07:27 | [CL_point_env] epoch #0 | Mean Task Embeddings (n_tasks x embedding_dim): tensor([[-0.2171, -0.6422, -0.0204, -0.2330,  0.6970],
        [ 0.2334,  0.6478,  0.0191,  0.2313, -0.6870]],
       grad_fn=<PermuteBackward0>)
-----------------------------  -----------
Embedding/MeanContrastiveLoss    0.0578935
PolicyTraining/MeanQ1Vals      -26.7977
PolicyTraining/MeanQ2Vals      -26.7977
PolicyTraining/MeanVVals       -25.6567
PolicyTraining/PolicyLoss       25.6114
PolicyTraining/QfLoss            0.485083
PolicyTraining/VfLoss            0.89481
-----------------------------  -----------
2025-01-08 09:07:27 | [CL_point_env] epoch #0 | Training Encoder

-----------------------------  -----------
Embedding/MeanContrastiveLoss    0.0578926
PolicyTraining/MeanQ1Vals      -30.7752
PolicyTraining/MeanQ2Vals      -30.7752
PolicyTraining/MeanVVals       -29.6161
PolicyTraining/PolicyLoss       29.6354
PolicyTraining/QfLoss            0.295163
PolicyTraining/VfLoss            0.113008
-----------------------------  -----------
-----------------------------  -----------
Embedding/MeanContrastiveLoss    0.0578926
PolicyTraining/MeanQ1Vals      -30.7752
PolicyTraining/MeanQ2Vals      -30.7752
PolicyTraining/MeanVVals       -29.6161
PolicyTraining/PolicyLoss       29.6354
PolicyTraining/QfLoss            0.295163
PolicyTraining/VfLoss            0.113008
-----------------------------  -----------
2025-01-08 09:07:47 | [CL_point_env] epoch #0 | Pairwise Similarity Matrix: tensor([[ 1.0000, -0.9999],
        [-0.9999,  1.0000]], grad_fn=<MmBackward0>)
2025-01-08 09:07:47 | [CL_point_env] epoch #0 | Mean Task Embeddings (n_tasks x embedding_dim): tensor([[-0.2189, -0.6432, -0.0187, -0.2304,  0.6964],
        [ 0.2293,  0.6472,  0.0209,  0.2318, -0.6887]],
       grad_fn=<PermuteBackward0>)
-----------------------------  -----------
Embedding/MeanContrastiveLoss    0.0578926
PolicyTraining/MeanQ1Vals      -30.7752
PolicyTraining/MeanQ2Vals      -30.7752
PolicyTraining/MeanVVals       -29.6161
PolicyTraining/PolicyLoss       29.6354
PolicyTraining/QfLoss            0.295163
PolicyTraining/VfLoss            0.113008
-----------------------------  -----------
2025-01-08 09:07:47 | [CL_point_env] epoch #0 | Training Encoder

-----------------------------  -----------
Embedding/MeanContrastiveLoss    0.0578928
PolicyTraining/MeanQ1Vals      -33.8309
PolicyTraining/MeanQ2Vals      -33.8309
PolicyTraining/MeanVVals       -32.7075
PolicyTraining/PolicyLoss       32.7265
PolicyTraining/QfLoss            0.208861
PolicyTraining/VfLoss            0.0758786
-----------------------------  -----------
-----------------------------  -----------
Embedding/MeanContrastiveLoss    0.0578928
PolicyTraining/MeanQ1Vals      -33.8309
PolicyTraining/MeanQ2Vals      -33.8309
PolicyTraining/MeanVVals       -32.7075
PolicyTraining/PolicyLoss       32.7265
PolicyTraining/QfLoss            0.208861
PolicyTraining/VfLoss            0.0758786
-----------------------------  -----------
2025-01-08 09:08:08 | [CL_point_env] epoch #0 | Pairwise Similarity Matrix: tensor([[ 1.0000, -1.0000],
        [-1.0000,  1.0000]], grad_fn=<MmBackward0>)
2025-01-08 09:08:08 | [CL_point_env] epoch #0 | Mean Task Embeddings (n_tasks x embedding_dim): tensor([[-0.2229, -0.6448, -0.0196, -0.2289,  0.6940],
        [ 0.2202,  0.6451,  0.0211,  0.2281, -0.6948]],
       grad_fn=<PermuteBackward0>)
-----------------------------  -----------
Embedding/MeanContrastiveLoss    0.0578928
PolicyTraining/MeanQ1Vals      -33.8309
PolicyTraining/MeanQ2Vals      -33.8309
PolicyTraining/MeanVVals       -32.7075
PolicyTraining/PolicyLoss       32.7265
PolicyTraining/QfLoss            0.208861
PolicyTraining/VfLoss            0.0758786
-----------------------------  -----------
2025-01-08 09:08:08 | [CL_point_env] epoch #0 | Training Encoder

-----------------------------  -----------
Embedding/MeanContrastiveLoss    0.0578884
PolicyTraining/MeanQ1Vals      -34.8297
PolicyTraining/MeanQ2Vals      -34.8297
PolicyTraining/MeanVVals       -33.7868
PolicyTraining/PolicyLoss       33.7824
PolicyTraining/QfLoss            0.136801
PolicyTraining/VfLoss            0.0723519
-----------------------------  -----------
-----------------------------  -----------
Embedding/MeanContrastiveLoss    0.0578884
PolicyTraining/MeanQ1Vals      -34.8297
PolicyTraining/MeanQ2Vals      -34.8297
PolicyTraining/MeanVVals       -33.7868
PolicyTraining/PolicyLoss       33.7824
PolicyTraining/QfLoss            0.136801
PolicyTraining/VfLoss            0.0723519
-----------------------------  -----------
2025-01-08 09:08:28 | [CL_point_env] epoch #0 | Pairwise Similarity Matrix: tensor([[ 1.0000, -0.9999],
        [-0.9999,  1.0000]], grad_fn=<MmBackward0>)
2025-01-08 09:08:28 | [CL_point_env] epoch #0 | Mean Task Embeddings (n_tasks x embedding_dim): tensor([[-0.2254, -0.6452, -0.0202, -0.2298,  0.6926],
        [ 0.2165,  0.6429,  0.0178,  0.2255, -0.6990]],
       grad_fn=<PermuteBackward0>)
-----------------------------  -----------
Embedding/MeanContrastiveLoss    0.0578884
PolicyTraining/MeanQ1Vals      -34.8297
PolicyTraining/MeanQ2Vals      -34.8297
PolicyTraining/MeanVVals       -33.7868
PolicyTraining/PolicyLoss       33.7824
PolicyTraining/QfLoss            0.136801
PolicyTraining/VfLoss            0.0723519
-----------------------------  -----------
2025-01-08 09:08:28 | [CL_point_env] epoch #0 | Training Encoder

-----------------------------  -----------
Embedding/MeanContrastiveLoss    0.0578893
PolicyTraining/MeanQ1Vals      -39.3191
PolicyTraining/MeanQ2Vals      -39.3191
PolicyTraining/MeanVVals       -38.3398
PolicyTraining/PolicyLoss       38.3496
PolicyTraining/QfLoss            0.105255
PolicyTraining/VfLoss            0.0640955
-----------------------------  -----------
-----------------------------  -----------
Embedding/MeanContrastiveLoss    0.0578893
PolicyTraining/MeanQ1Vals      -39.3191
PolicyTraining/MeanQ2Vals      -39.3191
PolicyTraining/MeanVVals       -38.3398
PolicyTraining/PolicyLoss       38.3496
PolicyTraining/QfLoss            0.105255
PolicyTraining/VfLoss            0.0640955
-----------------------------  -----------
2025-01-08 09:08:48 | [CL_point_env] epoch #0 | Pairwise Similarity Matrix: tensor([[ 1.0000, -1.0000],
        [-1.0000,  1.0000]], grad_fn=<MmBackward0>)
2025-01-08 09:08:48 | [CL_point_env] epoch #0 | Mean Task Embeddings (n_tasks x embedding_dim): tensor([[-0.2244, -0.6450, -0.0201, -0.2296,  0.6932],
        [ 0.2181,  0.6442,  0.0194,  0.2260, -0.6971]],
       grad_fn=<PermuteBackward0>)
-----------------------------  -----------
Embedding/MeanContrastiveLoss    0.0578893
PolicyTraining/MeanQ1Vals      -39.3191
PolicyTraining/MeanQ2Vals      -39.3191
PolicyTraining/MeanVVals       -38.3398
PolicyTraining/PolicyLoss       38.3496
PolicyTraining/QfLoss            0.105255
PolicyTraining/VfLoss            0.0640955
-----------------------------  -----------
2025-01-08 09:08:48 | [CL_point_env] epoch #0 | Training Encoder

-----------------------------  -----------
Embedding/MeanContrastiveLoss    0.0578889
PolicyTraining/MeanQ1Vals      -41.9295
PolicyTraining/MeanQ2Vals      -41.9295
PolicyTraining/MeanVVals       -41.0283
PolicyTraining/PolicyLoss       41.0142
PolicyTraining/QfLoss            0.10244
PolicyTraining/VfLoss            0.0648677
-----------------------------  -----------
-----------------------------  -----------
Embedding/MeanContrastiveLoss    0.0578889
PolicyTraining/MeanQ1Vals      -41.9295
PolicyTraining/MeanQ2Vals      -41.9295
PolicyTraining/MeanVVals       -41.0283
PolicyTraining/PolicyLoss       41.0142
PolicyTraining/QfLoss            0.10244
PolicyTraining/VfLoss            0.0648677
-----------------------------  -----------
2025-01-08 09:09:08 | [CL_point_env] epoch #0 | Pairwise Similarity Matrix: tensor([[ 1.0000, -1.0000],
        [-1.0000,  1.0000]], grad_fn=<MmBackward0>)
2025-01-08 09:09:08 | [CL_point_env] epoch #0 | Mean Task Embeddings (n_tasks x embedding_dim): tensor([[-0.2213, -0.6443, -0.0196, -0.2292,  0.6949],
        [ 0.2248,  0.6468,  0.0191,  0.2280, -0.6919]],
       grad_fn=<PermuteBackward0>)
-----------------------------  -----------
Embedding/MeanContrastiveLoss    0.0578889
PolicyTraining/MeanQ1Vals      -41.9295
PolicyTraining/MeanQ2Vals      -41.9295
PolicyTraining/MeanVVals       -41.0283
PolicyTraining/PolicyLoss       41.0142
PolicyTraining/QfLoss            0.10244
PolicyTraining/VfLoss            0.0648677
-----------------------------  -----------
2025-01-08 09:09:08 | [CL_point_env] epoch #0 | Training Encoder

-----------------------------  -----------
Embedding/MeanContrastiveLoss    0.0578864
PolicyTraining/MeanQ1Vals      -45.1105
PolicyTraining/MeanQ2Vals      -45.1105
PolicyTraining/MeanVVals       -44.2157
PolicyTraining/PolicyLoss       44.2664
PolicyTraining/QfLoss            0.0868079
PolicyTraining/VfLoss            0.0710359
-----------------------------  -----------
-----------------------------  -----------
Embedding/MeanContrastiveLoss    0.0578864
PolicyTraining/MeanQ1Vals      -45.1105
PolicyTraining/MeanQ2Vals      -45.1105
PolicyTraining/MeanVVals       -44.2157
PolicyTraining/PolicyLoss       44.2664
PolicyTraining/QfLoss            0.0868079
PolicyTraining/VfLoss            0.0710359
-----------------------------  -----------
2025-01-08 09:09:28 | [CL_point_env] epoch #0 | Pairwise Similarity Matrix: tensor([[ 1.0000, -1.0000],
        [-1.0000,  1.0000]], grad_fn=<MmBackward0>)
2025-01-08 09:09:28 | [CL_point_env] epoch #0 | Mean Task Embeddings (n_tasks x embedding_dim): tensor([[-0.2204, -0.6446, -0.0196, -0.2285,  0.6952],
        [ 0.2278,  0.6471,  0.0185,  0.2289, -0.6904]],
       grad_fn=<PermuteBackward0>)
-----------------------------  -----------
Embedding/MeanContrastiveLoss    0.0578864
PolicyTraining/MeanQ1Vals      -45.1105
PolicyTraining/MeanQ2Vals      -45.1105
PolicyTraining/MeanVVals       -44.2157
PolicyTraining/PolicyLoss       44.2664
PolicyTraining/QfLoss            0.0868079
PolicyTraining/VfLoss            0.0710359
-----------------------------  -----------
2025-01-08 09:09:28 | [CL_point_env] epoch #0 | Training Encoder

-----------------------------  -----------
Embedding/MeanContrastiveLoss    0.0578875
PolicyTraining/MeanQ1Vals      -51.35
PolicyTraining/MeanQ2Vals      -51.35
PolicyTraining/MeanVVals       -50.6032
PolicyTraining/PolicyLoss       50.6139
PolicyTraining/QfLoss            0.0782334
PolicyTraining/VfLoss            0.0590598
-----------------------------  -----------
-----------------------------  -----------
Embedding/MeanContrastiveLoss    0.0578875
PolicyTraining/MeanQ1Vals      -51.35
PolicyTraining/MeanQ2Vals      -51.35
PolicyTraining/MeanVVals       -50.6032
PolicyTraining/PolicyLoss       50.6139
PolicyTraining/QfLoss            0.0782334
PolicyTraining/VfLoss            0.0590598
-----------------------------  -----------
2025-01-08 09:09:48 | [CL_point_env] epoch #0 | Pairwise Similarity Matrix: tensor([[ 1.0000, -1.0000],
        [-1.0000,  1.0000]], grad_fn=<MmBackward0>)
2025-01-08 09:09:48 | [CL_point_env] epoch #0 | Mean Task Embeddings (n_tasks x embedding_dim): tensor([[-0.2216, -0.6454, -0.0194, -0.2277,  0.6943],
        [ 0.2241,  0.6451,  0.0189,  0.2291, -0.6933]],
       grad_fn=<PermuteBackward0>)
-----------------------------  -----------
Embedding/MeanContrastiveLoss    0.0578875
PolicyTraining/MeanQ1Vals      -51.35
PolicyTraining/MeanQ2Vals      -51.35
PolicyTraining/MeanVVals       -50.6032
PolicyTraining/PolicyLoss       50.6139
PolicyTraining/QfLoss            0.0782334
PolicyTraining/VfLoss            0.0590598
-----------------------------  -----------
2025-01-08 09:09:48 | [CL_point_env] epoch #0 | Training Encoder

-----------------------------  -----------
Embedding/MeanContrastiveLoss    0.0578864
PolicyTraining/MeanQ1Vals      -55.1313
PolicyTraining/MeanQ2Vals      -55.1313
PolicyTraining/MeanVVals       -54.4172
PolicyTraining/PolicyLoss       54.4564
PolicyTraining/QfLoss            0.0725208
PolicyTraining/VfLoss            0.069953
-----------------------------  -----------
-----------------------------  -----------
Embedding/MeanContrastiveLoss    0.0578864
PolicyTraining/MeanQ1Vals      -55.1313
PolicyTraining/MeanQ2Vals      -55.1313
PolicyTraining/MeanVVals       -54.4172
PolicyTraining/PolicyLoss       54.4564
PolicyTraining/QfLoss            0.0725208
PolicyTraining/VfLoss            0.069953
-----------------------------  -----------
2025-01-08 09:10:08 | [CL_point_env] epoch #0 | Pairwise Similarity Matrix: tensor([[ 1.0000, -1.0000],
        [-1.0000,  1.0000]], grad_fn=<MmBackward0>)
2025-01-08 09:10:08 | [CL_point_env] epoch #0 | Mean Task Embeddings (n_tasks x embedding_dim): tensor([[-0.2237, -0.6449, -0.0208, -0.2299,  0.6934],
        [ 0.2208,  0.6450,  0.0178,  0.2247, -0.6960]],
       grad_fn=<PermuteBackward0>)
-----------------------------  -----------
Embedding/MeanContrastiveLoss    0.0578864
PolicyTraining/MeanQ1Vals      -55.1313
PolicyTraining/MeanQ2Vals      -55.1313
PolicyTraining/MeanVVals       -54.4172
PolicyTraining/PolicyLoss       54.4564
PolicyTraining/QfLoss            0.0725208
PolicyTraining/VfLoss            0.069953
-----------------------------  -----------
2025-01-08 09:10:08 | [CL_point_env] epoch #0 | Training Encoder

-----------------------------  -----------
Embedding/MeanContrastiveLoss    0.0578858
PolicyTraining/MeanQ1Vals      -56.8077
PolicyTraining/MeanQ2Vals      -56.8077
PolicyTraining/MeanVVals       -56.3087
PolicyTraining/PolicyLoss       56.2309
PolicyTraining/QfLoss            0.0737983
PolicyTraining/VfLoss            0.0646639
-----------------------------  -----------
-----------------------------  -----------
Embedding/MeanContrastiveLoss    0.0578858
PolicyTraining/MeanQ1Vals      -56.8077
PolicyTraining/MeanQ2Vals      -56.8077
PolicyTraining/MeanVVals       -56.3087
PolicyTraining/PolicyLoss       56.2309
PolicyTraining/QfLoss            0.0737983
PolicyTraining/VfLoss            0.0646639
-----------------------------  -----------
2025-01-08 09:10:28 | [CL_point_env] epoch #0 | Pairwise Similarity Matrix: tensor([[ 1.0000, -1.0000],
        [-1.0000,  1.0000]], grad_fn=<MmBackward0>)
2025-01-08 09:10:28 | [CL_point_env] epoch #0 | Mean Task Embeddings (n_tasks x embedding_dim): tensor([[-0.2241, -0.6454, -0.0199, -0.2284,  0.6933],
        [ 0.2189,  0.6449,  0.0201,  0.2275, -0.6957]],
       grad_fn=<PermuteBackward0>)
-----------------------------  -----------
Embedding/MeanContrastiveLoss    0.0578858
PolicyTraining/MeanQ1Vals      -56.8077
PolicyTraining/MeanQ2Vals      -56.8077
PolicyTraining/MeanVVals       -56.3087
PolicyTraining/PolicyLoss       56.2309
PolicyTraining/QfLoss            0.0737983
PolicyTraining/VfLoss            0.0646639
-----------------------------  -----------
2025-01-08 09:10:28 | [CL_point_env] epoch #0 | Training Encoder

-----------------------------  -----------
Embedding/MeanContrastiveLoss    0.0578864
PolicyTraining/MeanQ1Vals      -59.0754
PolicyTraining/MeanQ2Vals      -59.0754
PolicyTraining/MeanVVals       -58.5411
PolicyTraining/PolicyLoss       58.5598
PolicyTraining/QfLoss            0.0779131
PolicyTraining/VfLoss            0.0692206
-----------------------------  -----------
-----------------------------  -----------
Embedding/MeanContrastiveLoss    0.0578864
PolicyTraining/MeanQ1Vals      -59.0754
PolicyTraining/MeanQ2Vals      -59.0754
PolicyTraining/MeanVVals       -58.5411
PolicyTraining/PolicyLoss       58.5598
PolicyTraining/QfLoss            0.0779131
PolicyTraining/VfLoss            0.0692206
-----------------------------  -----------
2025-01-08 09:10:48 | [CL_point_env] epoch #0 | Pairwise Similarity Matrix: tensor([[ 1.0000, -1.0000],
        [-1.0000,  1.0000]], grad_fn=<MmBackward0>)
2025-01-08 09:10:48 | [CL_point_env] epoch #0 | Mean Task Embeddings (n_tasks x embedding_dim): tensor([[-0.2228, -0.6450, -0.0197, -0.2281,  0.6942],
        [ 0.2233,  0.6456,  0.0210,  0.2267, -0.6939]],
       grad_fn=<PermuteBackward0>)
-----------------------------  -----------
Embedding/MeanContrastiveLoss    0.0578864
PolicyTraining/MeanQ1Vals      -59.0754
PolicyTraining/MeanQ2Vals      -59.0754
PolicyTraining/MeanVVals       -58.5411
PolicyTraining/PolicyLoss       58.5598
PolicyTraining/QfLoss            0.0779131
PolicyTraining/VfLoss            0.0692206
-----------------------------  -----------
2025-01-08 09:10:48 | [CL_point_env] epoch #0 | Training Encoder

-----------------------------  -----------
Embedding/MeanContrastiveLoss    0.0578852
PolicyTraining/MeanQ1Vals      -62.6757
PolicyTraining/MeanQ2Vals      -62.6757
PolicyTraining/MeanVVals       -62.2326
PolicyTraining/PolicyLoss       62.2317
PolicyTraining/QfLoss            0.0677262
PolicyTraining/VfLoss            0.0717044
-----------------------------  -----------
-----------------------------  -----------
Embedding/MeanContrastiveLoss    0.0578852
PolicyTraining/MeanQ1Vals      -62.6757
PolicyTraining/MeanQ2Vals      -62.6757
PolicyTraining/MeanVVals       -62.2326
PolicyTraining/PolicyLoss       62.2317
PolicyTraining/QfLoss            0.0677262
PolicyTraining/VfLoss            0.0717044
-----------------------------  -----------
2025-01-08 09:11:07 | [CL_point_env] epoch #0 | Evaluating...
2025-01-08 09:11:07 | [CL_point_env] epoch #0 | Sampling for adapation and meta-testing...


Failed to import TF-Keras. Please note that TF-Keras is not installed by default when you install TensorFlow Probability. This is so that JAX-only users do not have to install TensorFlow or TF-Keras. To use TensorFlow Probability with TensorFlow, please install the tf-keras or tf-keras-nightly package.
This can be be done through installing the tensorflow-probability[tf] extra.
/Users/paulnitschke/Desktop/projects/geo_meta_rl/garage/experiment/deterministic.py:36: UserWarning: Enabeling deterministic mode in PyTorch can have a performance impact when using GPU.
  warnings.warn(


2025-01-08 09:11:08 | [CL_point_env] epoch #0 | Finished meta-testing...
2025-01-08 09:11:08 | [CL_point_env] epoch #0 | Sampling for adapation and meta-testing...


Failed to import TF-Keras. Please note that TF-Keras is not installed by default when you install TensorFlow Probability. This is so that JAX-only users do not have to install TensorFlow or TF-Keras. To use TensorFlow Probability with TensorFlow, please install the tf-keras or tf-keras-nightly package.
This can be be done through installing the tensorflow-probability[tf] extra.


2025-01-08 09:11:17 | [CL_point_env] epoch #0 | Finished meta-testing...
2025-01-08 09:11:17 | [CL_point_env] epoch #0 | Saving snapshot...
2025-01-08 09:11:17 | [CL_point_env] epoch #0 | Saved
2025-01-08 09:11:17 | [CL_point_env] epoch #0 | Time 330.81 s
2025-01-08 09:11:17 | [CL_point_env] epoch #0 | EpochTime 330.80 s
--------------------------------------------------  ------------
Embedding/MeanContrastiveLoss                          0.0578852
MetaTest/Average/AverageDiscountedReturn            -480.349
MetaTest/Average/AverageReturn                      -480.349
MetaTest/Average/Iteration                             0
MetaTest/Average/MaxReturn                          -286.006
MetaTest/Average/MinReturn                          -563.339
MetaTest/Average/NumEpisodes                          20
MetaTest/Average/StdReturn                            83.5783
MetaTest/Average/SuccessRate                           0
MetaTest/Average/TerminationRate                       0
MetaTest/__unnamed_task__/AverageDiscountedReturn   -480.349
MetaTest/__unnamed_task__/AverageReturn             -480.349
MetaTest/__unnamed_task__/Iteration                    0
MetaTest/__unnamed_task__/MaxReturn                 -286.006
MetaTest/__unnamed_task__/MinReturn                 -563.339
MetaTest/__unnamed_task__/NumEpisodes                 20
MetaTest/__unnamed_task__/StdReturn                   83.5783
MetaTest/__unnamed_task__/SuccessRate                  0
MetaTest/__unnamed_task__/TerminationRate              0
MetaTrain/Average/AverageDiscountedReturn           -557.496
MetaTrain/Average/AverageReturn                     -557.496
MetaTrain/Average/Iteration                            0
MetaTrain/Average/MaxReturn                         -551.652
MetaTrain/Average/MinReturn                         -563.339
MetaTrain/Average/NumEpisodes                          2
MetaTrain/Average/StdReturn                            5.84347
MetaTrain/Average/SuccessRate                          0
MetaTrain/Average/TerminationRate                      0
MetaTrain/__unnamed_task__/AverageDiscountedReturn  -557.496
MetaTrain/__unnamed_task__/AverageReturn            -557.496
MetaTrain/__unnamed_task__/Iteration                   0
MetaTrain/__unnamed_task__/MaxReturn                -551.652
MetaTrain/__unnamed_task__/MinReturn                -563.339
MetaTrain/__unnamed_task__/NumEpisodes                 2
MetaTrain/__unnamed_task__/StdReturn                   5.84347
MetaTrain/__unnamed_task__/SuccessRate                 0
MetaTrain/__unnamed_task__/TerminationRate             0
PolicyTraining/MeanQ1Vals                            -62.6757
PolicyTraining/MeanQ2Vals                            -62.6757
PolicyTraining/MeanVVals                             -62.2326
PolicyTraining/PolicyLoss                             62.2317
PolicyTraining/QfLoss                                  0.0677262
PolicyTraining/VfLoss                                  0.0717044
TotalEnvSteps                                       6900
--------------------------------------------------  ------------
2025-01-08 09:11:19 | [CL_point_env] epoch #1 | Training...
2025-01-08 09:11:22 | [CL_point_env] epoch #1 | Pairwise Similarity Matrix: tensor([[ 1.0000, -0.9910],
        [-0.9910,  1.0000]], grad_fn=<MmBackward0>)
2025-01-08 09:11:22 | [CL_point_env] epoch #1 | Mean Task Embeddings (n_tasks x embedding_dim): tensor([[ 0.2663,  0.6365,  0.0870,  0.2306, -0.6806],
        [-0.2234, -0.6423,  0.0235, -0.1762,  0.7113]],
       grad_fn=<PermuteBackward0>)

2025-01-08 09:11:22 | [CL_point_env] epoch #1 | Training Encoder

-----------------------------  ----------
Embedding/MeanContrastiveLoss    0.226217
PolicyTraining/MeanQ1Vals      -79.6444
PolicyTraining/MeanQ2Vals      -79.6444
PolicyTraining/MeanVVals       -78.837
PolicyTraining/PolicyLoss       78.8873
PolicyTraining/QfLoss            0.992861
PolicyTraining/VfLoss            0.309821
-----------------------------  ----------
-----------------------------  ----------
Embedding/MeanContrastiveLoss    0.226217
PolicyTraining/MeanQ1Vals      -79.6444
PolicyTraining/MeanQ2Vals      -79.6444
PolicyTraining/MeanVVals       -78.837
PolicyTraining/PolicyLoss       78.8873
PolicyTraining/QfLoss            0.992861
PolicyTraining/VfLoss            0.309821
-----------------------------  ----------
2025-01-08 09:11:43 | [CL_point_env] epoch #1 | Pairwise Similarity Matrix: tensor([[ 1.0000, -0.9725],
        [-0.9725,  1.0000]], grad_fn=<MmBackward0>)
2025-01-08 09:11:43 | [CL_point_env] epoch #1 | Mean Task Embeddings (n_tasks x embedding_dim): tensor([[ 0.2590,  0.6450,  0.1153,  0.2743, -0.6545],
        [-0.2318, -0.6539,  0.0429, -0.1129,  0.7099]],
       grad_fn=<PermuteBackward0>)
-----------------------------  ----------
Embedding/MeanContrastiveLoss    0.226217
PolicyTraining/MeanQ1Vals      -79.6444
PolicyTraining/MeanQ2Vals      -79.6444
PolicyTraining/MeanVVals       -78.837
PolicyTraining/PolicyLoss       78.8873
PolicyTraining/QfLoss            0.992861
PolicyTraining/VfLoss            0.309821
-----------------------------  ----------
2025-01-08 09:11:43 | [CL_point_env] epoch #1 | Training Encoder

-----------------------------  ----------
Embedding/MeanContrastiveLoss    0.117733
PolicyTraining/MeanQ1Vals      -84.71
PolicyTraining/MeanQ2Vals      -84.71
PolicyTraining/MeanVVals       -84.007
PolicyTraining/PolicyLoss       84.0439
PolicyTraining/QfLoss            1.10192
PolicyTraining/VfLoss            0.158689
-----------------------------  ----------
-----------------------------  ----------
Embedding/MeanContrastiveLoss    0.117733
PolicyTraining/MeanQ1Vals      -84.71
PolicyTraining/MeanQ2Vals      -84.71
PolicyTraining/MeanVVals       -84.007
PolicyTraining/PolicyLoss       84.0439
PolicyTraining/QfLoss            1.10192
PolicyTraining/VfLoss            0.158689
-----------------------------  ----------
2025-01-08 09:12:05 | [CL_point_env] epoch #1 | Pairwise Similarity Matrix: tensor([[ 1.0000, -0.9988],
        [-0.9988,  1.0000]], grad_fn=<MmBackward0>)
2025-01-08 09:12:05 | [CL_point_env] epoch #1 | Mean Task Embeddings (n_tasks x embedding_dim): tensor([[ 0.2531,  0.6494,  0.0528,  0.2358, -0.6751],
        [-0.2778, -0.6528, -0.0877, -0.2156,  0.6652]],
       grad_fn=<PermuteBackward0>)
-----------------------------  ----------
Embedding/MeanContrastiveLoss    0.117733
PolicyTraining/MeanQ1Vals      -84.71
PolicyTraining/MeanQ2Vals      -84.71
PolicyTraining/MeanVVals       -84.007
PolicyTraining/PolicyLoss       84.0439
PolicyTraining/QfLoss            1.10192
PolicyTraining/VfLoss            0.158689
-----------------------------  ----------
2025-01-08 09:12:05 | [CL_point_env] epoch #1 | Training Encoder

-----------------------------  ----------
Embedding/MeanContrastiveLoss    0.135813
PolicyTraining/MeanQ1Vals      -88.6458
PolicyTraining/MeanQ2Vals      -88.6458
PolicyTraining/MeanVVals       -87.9817
PolicyTraining/PolicyLoss       88.0424
PolicyTraining/QfLoss            1.6106
PolicyTraining/VfLoss            0.166872
-----------------------------  ----------
-----------------------------  ----------
Embedding/MeanContrastiveLoss    0.135813
PolicyTraining/MeanQ1Vals      -88.6458
PolicyTraining/MeanQ2Vals      -88.6458
PolicyTraining/MeanVVals       -87.9817
PolicyTraining/PolicyLoss       88.0424
PolicyTraining/QfLoss            1.6106
PolicyTraining/VfLoss            0.166872
-----------------------------  ----------
2025-01-08 09:12:25 | [CL_point_env] epoch #1 | Pairwise Similarity Matrix: tensor([[ 1.0000, -0.9963],
        [-0.9963,  1.0000]], grad_fn=<MmBackward0>)
2025-01-08 09:12:25 | [CL_point_env] epoch #1 | Mean Task Embeddings (n_tasks x embedding_dim): tensor([[ 0.2556,  0.6506,  0.0394,  0.2167, -0.6804],
        [-0.2758, -0.6462, -0.1070, -0.2576,  0.6546]],
       grad_fn=<PermuteBackward0>)
-----------------------------  ----------
Embedding/MeanContrastiveLoss    0.135813
PolicyTraining/MeanQ1Vals      -88.6458
PolicyTraining/MeanQ2Vals      -88.6458
PolicyTraining/MeanVVals       -87.9817
PolicyTraining/PolicyLoss       88.0424
PolicyTraining/QfLoss            1.6106
PolicyTraining/VfLoss            0.166872
-----------------------------  ----------
2025-01-08 09:12:25 | [CL_point_env] epoch #1 | Training Encoder

-----------------------------  -----------
Embedding/MeanContrastiveLoss    0.0579877
PolicyTraining/MeanQ1Vals      -94.1924
PolicyTraining/MeanQ2Vals      -94.1924
PolicyTraining/MeanVVals       -93.6821
PolicyTraining/PolicyLoss       93.6607
PolicyTraining/QfLoss            0.645092
PolicyTraining/VfLoss            0.140479
-----------------------------  -----------
-----------------------------  -----------
Embedding/MeanContrastiveLoss    0.0579877
PolicyTraining/MeanQ1Vals      -94.1924
PolicyTraining/MeanQ2Vals      -94.1924
PolicyTraining/MeanVVals       -93.6821
PolicyTraining/PolicyLoss       93.6607
PolicyTraining/QfLoss            0.645092
PolicyTraining/VfLoss            0.140479
-----------------------------  -----------
2025-01-08 09:12:46 | [CL_point_env] epoch #1 | Pairwise Similarity Matrix: tensor([[ 1.0000, -0.9957],
        [-0.9957,  1.0000]], grad_fn=<MmBackward0>)
2025-01-08 09:12:46 | [CL_point_env] epoch #1 | Mean Task Embeddings (n_tasks x embedding_dim): tensor([[ 0.2605,  0.6514,  0.0409,  0.2073, -0.6805],
        [-0.2659, -0.6415, -0.0984, -0.2750,  0.6576]],
       grad_fn=<PermuteBackward0>)
-----------------------------  -----------
Embedding/MeanContrastiveLoss    0.0579877
PolicyTraining/MeanQ1Vals      -94.1924
PolicyTraining/MeanQ2Vals      -94.1924
PolicyTraining/MeanVVals       -93.6821
PolicyTraining/PolicyLoss       93.6607
PolicyTraining/QfLoss            0.645092
PolicyTraining/VfLoss            0.140479
-----------------------------  -----------
2025-01-08 09:12:46 | [CL_point_env] epoch #1 | Training Encoder

-----------------------------  -----------
Embedding/MeanContrastiveLoss    0.0580483
PolicyTraining/MeanQ1Vals      -97.008
PolicyTraining/MeanQ2Vals      -97.008
PolicyTraining/MeanVVals       -96.4804
PolicyTraining/PolicyLoss       96.5384
PolicyTraining/QfLoss            0.605204
PolicyTraining/VfLoss            0.143258
-----------------------------  -----------
-----------------------------  -----------
Embedding/MeanContrastiveLoss    0.0580483
PolicyTraining/MeanQ1Vals      -97.008
PolicyTraining/MeanQ2Vals      -97.008
PolicyTraining/MeanVVals       -96.4804
PolicyTraining/PolicyLoss       96.5384
PolicyTraining/QfLoss            0.605204
PolicyTraining/VfLoss            0.143258
-----------------------------  -----------
2025-01-08 09:13:06 | [CL_point_env] epoch #1 | Pairwise Similarity Matrix: tensor([[ 1.0000, -0.9978],
        [-0.9978,  1.0000]], grad_fn=<MmBackward0>)
2025-01-08 09:13:06 | [CL_point_env] epoch #1 | Mean Task Embeddings (n_tasks x embedding_dim): tensor([[ 0.2642,  0.6520,  0.0514,  0.2097, -0.6771],
        [-0.2566, -0.6419, -0.0769, -0.2686,  0.6663]],
       grad_fn=<PermuteBackward0>)
-----------------------------  -----------
Embedding/MeanContrastiveLoss    0.0580483
PolicyTraining/MeanQ1Vals      -97.008
PolicyTraining/MeanQ2Vals      -97.008
PolicyTraining/MeanVVals       -96.4804
PolicyTraining/PolicyLoss       96.5384
PolicyTraining/QfLoss            0.605204
PolicyTraining/VfLoss            0.143258
-----------------------------  -----------
2025-01-08 09:13:06 | [CL_point_env] epoch #1 | Training Encoder

-----------------------------  -----------
Embedding/MeanContrastiveLoss     0.057995
PolicyTraining/MeanQ1Vals      -103.593
PolicyTraining/MeanQ2Vals      -103.593
PolicyTraining/MeanVVals       -103.338
PolicyTraining/PolicyLoss       103.207
PolicyTraining/QfLoss             0.904506
PolicyTraining/VfLoss             0.157135
-----------------------------  -----------
-----------------------------  -----------
Embedding/MeanContrastiveLoss     0.057995
PolicyTraining/MeanQ1Vals      -103.593
PolicyTraining/MeanQ2Vals      -103.593
PolicyTraining/MeanVVals       -103.338
PolicyTraining/PolicyLoss       103.207
PolicyTraining/QfLoss             0.904506
PolicyTraining/VfLoss             0.157135
-----------------------------  -----------
2025-01-08 09:13:26 | [CL_point_env] epoch #1 | Pairwise Similarity Matrix: tensor([[ 1.0000, -0.9994],
        [-0.9994,  1.0000]], grad_fn=<MmBackward0>)
2025-01-08 09:13:26 | [CL_point_env] epoch #1 | Mean Task Embeddings (n_tasks x embedding_dim): tensor([[ 0.2659,  0.6524,  0.0637,  0.2199, -0.6718],
        [-0.2521, -0.6450, -0.0572, -0.2516,  0.6737]],
       grad_fn=<PermuteBackward0>)
-----------------------------  -----------
Embedding/MeanContrastiveLoss     0.057995
PolicyTraining/MeanQ1Vals      -103.593
PolicyTraining/MeanQ2Vals      -103.593
PolicyTraining/MeanVVals       -103.338
PolicyTraining/PolicyLoss       103.207
PolicyTraining/QfLoss             0.904506
PolicyTraining/VfLoss             0.157135
-----------------------------  -----------
2025-01-08 09:13:26 | [CL_point_env] epoch #1 | Training Encoder

-----------------------------  ------------
Embedding/MeanContrastiveLoss     0.0579231
PolicyTraining/MeanQ1Vals      -107.724
PolicyTraining/MeanQ2Vals      -107.724
PolicyTraining/MeanVVals       -107.601
PolicyTraining/PolicyLoss       107.448
PolicyTraining/QfLoss             0.769208
PolicyTraining/VfLoss             0.158006
-----------------------------  ------------
-----------------------------  ------------
Embedding/MeanContrastiveLoss     0.0579231
PolicyTraining/MeanQ1Vals      -107.724
PolicyTraining/MeanQ2Vals      -107.724
PolicyTraining/MeanVVals       -107.601
PolicyTraining/PolicyLoss       107.448
PolicyTraining/QfLoss             0.769208
PolicyTraining/VfLoss             0.158006
-----------------------------  ------------
2025-01-08 09:13:46 | [CL_point_env] epoch #1 | Pairwise Similarity Matrix: tensor([[ 1.0000, -0.9996],
        [-0.9996,  1.0000]], grad_fn=<MmBackward0>)
2025-01-08 09:13:46 | [CL_point_env] epoch #1 | Mean Task Embeddings (n_tasks x embedding_dim): tensor([[ 0.2647,  0.6521,  0.0713,  0.2325, -0.6675],
        [-0.2532, -0.6489, -0.0471, -0.2332,  0.6769]],
       grad_fn=<PermuteBackward0>)
-----------------------------  ------------
Embedding/MeanContrastiveLoss     0.0579231
PolicyTraining/MeanQ1Vals      -107.724
PolicyTraining/MeanQ2Vals      -107.724
PolicyTraining/MeanVVals       -107.601
PolicyTraining/PolicyLoss       107.448
PolicyTraining/QfLoss             0.769208
PolicyTraining/VfLoss             0.158006
-----------------------------  ------------
2025-01-08 09:13:46 | [CL_point_env] epoch #1 | Training Encoder

-----------------------------  ------------
Embedding/MeanContrastiveLoss     0.0578989
PolicyTraining/MeanQ1Vals      -111.716
PolicyTraining/MeanQ2Vals      -111.716
PolicyTraining/MeanVVals       -111.517
PolicyTraining/PolicyLoss       111.458
PolicyTraining/QfLoss             0.915689
PolicyTraining/VfLoss             0.14947
-----------------------------  ------------
-----------------------------  ------------
Embedding/MeanContrastiveLoss     0.0578989
PolicyTraining/MeanQ1Vals      -111.716
PolicyTraining/MeanQ2Vals      -111.716
PolicyTraining/MeanVVals       -111.517
PolicyTraining/PolicyLoss       111.458
PolicyTraining/QfLoss             0.915689
PolicyTraining/VfLoss             0.14947
-----------------------------  ------------
2025-01-08 09:14:07 | [CL_point_env] epoch #1 | Pairwise Similarity Matrix: tensor([[ 1.0000, -0.9995],
        [-0.9995,  1.0000]], grad_fn=<MmBackward0>)
2025-01-08 09:14:07 | [CL_point_env] epoch #1 | Mean Task Embeddings (n_tasks x embedding_dim): tensor([[ 0.2615,  0.6512,  0.0722,  0.2414, -0.6664],
        [-0.2584, -0.6527, -0.0491, -0.2201,  0.6756]],
       grad_fn=<PermuteBackward0>)
-----------------------------  ------------
Embedding/MeanContrastiveLoss     0.0578989
PolicyTraining/MeanQ1Vals      -111.716
PolicyTraining/MeanQ2Vals      -111.716
PolicyTraining/MeanVVals       -111.517
PolicyTraining/PolicyLoss       111.458
PolicyTraining/QfLoss             0.915689
PolicyTraining/VfLoss             0.14947
-----------------------------  ------------
2025-01-08 09:14:07 | [CL_point_env] epoch #1 | Training Encoder

-----------------------------  -----------
Embedding/MeanContrastiveLoss     0.057903
PolicyTraining/MeanQ1Vals      -116.081
PolicyTraining/MeanQ2Vals      -116.081
PolicyTraining/MeanVVals       -115.771
PolicyTraining/PolicyLoss       115.837
PolicyTraining/QfLoss             0.71346
PolicyTraining/VfLoss             0.159768
-----------------------------  -----------
-----------------------------  -----------
Embedding/MeanContrastiveLoss     0.057903
PolicyTraining/MeanQ1Vals      -116.081
PolicyTraining/MeanQ2Vals      -116.081
PolicyTraining/MeanVVals       -115.771
PolicyTraining/PolicyLoss       115.837
PolicyTraining/QfLoss             0.71346
PolicyTraining/VfLoss             0.159768
-----------------------------  -----------
2025-01-08 09:14:27 | [CL_point_env] epoch #1 | Pairwise Similarity Matrix: tensor([[ 1.0000, -0.9995],
        [-0.9995,  1.0000]], grad_fn=<MmBackward0>)
2025-01-08 09:14:27 | [CL_point_env] epoch #1 | Mean Task Embeddings (n_tasks x embedding_dim): tensor([[ 0.2584,  0.6500,  0.0683,  0.2439, -0.6683],
        [-0.2630, -0.6547, -0.0565, -0.2157,  0.6726]],
       grad_fn=<PermuteBackward0>)
-----------------------------  -----------
Embedding/MeanContrastiveLoss     0.057903
PolicyTraining/MeanQ1Vals      -116.081
PolicyTraining/MeanQ2Vals      -116.081
PolicyTraining/MeanVVals       -115.771
PolicyTraining/PolicyLoss       115.837
PolicyTraining/QfLoss             0.71346
PolicyTraining/VfLoss             0.159768
-----------------------------  -----------
2025-01-08 09:14:27 | [CL_point_env] epoch #1 | Training Encoder

-----------------------------  ------------
Embedding/MeanContrastiveLoss     0.0579028
PolicyTraining/MeanQ1Vals      -121.041
PolicyTraining/MeanQ2Vals      -121.041
PolicyTraining/MeanVVals       -120.952
PolicyTraining/PolicyLoss       120.933
PolicyTraining/QfLoss             0.843346
PolicyTraining/VfLoss             0.179578
-----------------------------  ------------
-----------------------------  ------------
Embedding/MeanContrastiveLoss     0.0579028
PolicyTraining/MeanQ1Vals      -121.041
PolicyTraining/MeanQ2Vals      -121.041
PolicyTraining/MeanVVals       -120.952
PolicyTraining/PolicyLoss       120.933
PolicyTraining/QfLoss             0.843346
PolicyTraining/VfLoss             0.179578
-----------------------------  ------------
2025-01-08 09:14:47 | [CL_point_env] epoch #1 | Pairwise Similarity Matrix: tensor([[ 1.0000, -0.9997],
        [-0.9997,  1.0000]], grad_fn=<MmBackward0>)
2025-01-08 09:14:47 | [CL_point_env] epoch #1 | Mean Task Embeddings (n_tasks x embedding_dim): tensor([[ 0.2573,  0.6493,  0.0635,  0.2414, -0.6707],
        [-0.2657, -0.6543, -0.0642, -0.2195,  0.6701]],
       grad_fn=<PermuteBackward0>)
-----------------------------  ------------
Embedding/MeanContrastiveLoss     0.0579028
PolicyTraining/MeanQ1Vals      -121.041
PolicyTraining/MeanQ2Vals      -121.041
PolicyTraining/MeanVVals       -120.952
PolicyTraining/PolicyLoss       120.933
PolicyTraining/QfLoss             0.843346
PolicyTraining/VfLoss             0.179578
-----------------------------  ------------
2025-01-08 09:14:47 | [CL_point_env] epoch #1 | Training Encoder

-----------------------------  ------------
Embedding/MeanContrastiveLoss     0.0578957
PolicyTraining/MeanQ1Vals      -124.664
PolicyTraining/MeanQ2Vals      -124.664
PolicyTraining/MeanVVals       -124.557
PolicyTraining/PolicyLoss       124.627
PolicyTraining/QfLoss             0.775353
PolicyTraining/VfLoss             0.157794
-----------------------------  ------------
-----------------------------  ------------
Embedding/MeanContrastiveLoss     0.0578957
PolicyTraining/MeanQ1Vals      -124.664
PolicyTraining/MeanQ2Vals      -124.664
PolicyTraining/MeanVVals       -124.557
PolicyTraining/PolicyLoss       124.627
PolicyTraining/QfLoss             0.775353
PolicyTraining/VfLoss             0.157794
-----------------------------  ------------
2025-01-08 09:15:07 | [CL_point_env] epoch #1 | Pairwise Similarity Matrix: tensor([[ 1.0000, -0.9999],
        [-0.9999,  1.0000]], grad_fn=<MmBackward0>)
2025-01-08 09:15:07 | [CL_point_env] epoch #1 | Mean Task Embeddings (n_tasks x embedding_dim): tensor([[ 0.2578,  0.6492,  0.0604,  0.2366, -0.6726],
        [-0.2647, -0.6530, -0.0678, -0.2265,  0.6691]],
       grad_fn=<PermuteBackward0>)
-----------------------------  ------------
Embedding/MeanContrastiveLoss     0.0578957
PolicyTraining/MeanQ1Vals      -124.664
PolicyTraining/MeanQ2Vals      -124.664
PolicyTraining/MeanVVals       -124.557
PolicyTraining/PolicyLoss       124.627
PolicyTraining/QfLoss             0.775353
PolicyTraining/VfLoss             0.157794
-----------------------------  ------------
2025-01-08 09:15:07 | [CL_point_env] epoch #1 | Training Encoder

-----------------------------  ------------
Embedding/MeanContrastiveLoss     0.0578888
PolicyTraining/MeanQ1Vals      -128.671
PolicyTraining/MeanQ2Vals      -128.671
PolicyTraining/MeanVVals       -128.608
PolicyTraining/PolicyLoss       128.696
PolicyTraining/QfLoss             0.64451
PolicyTraining/VfLoss             0.167574
-----------------------------  ------------
-----------------------------  ------------
Embedding/MeanContrastiveLoss     0.0578888
PolicyTraining/MeanQ1Vals      -128.671
PolicyTraining/MeanQ2Vals      -128.671
PolicyTraining/MeanVVals       -128.608
PolicyTraining/PolicyLoss       128.696
PolicyTraining/QfLoss             0.64451
PolicyTraining/VfLoss             0.167574
-----------------------------  ------------
2025-01-08 09:15:27 | [CL_point_env] epoch #1 | Pairwise Similarity Matrix: tensor([[ 1.0000, -0.9999],
        [-0.9999,  1.0000]], grad_fn=<MmBackward0>)
2025-01-08 09:15:27 | [CL_point_env] epoch #1 | Mean Task Embeddings (n_tasks x embedding_dim): tensor([[ 0.2591,  0.6496,  0.0595,  0.2320, -0.6734],
        [-0.2628, -0.6513, -0.0684, -0.2335,  0.6690]],
       grad_fn=<PermuteBackward0>)
-----------------------------  ------------
Embedding/MeanContrastiveLoss     0.0578888
PolicyTraining/MeanQ1Vals      -128.671
PolicyTraining/MeanQ2Vals      -128.671
PolicyTraining/MeanVVals       -128.608
PolicyTraining/PolicyLoss       128.696
PolicyTraining/QfLoss             0.64451
PolicyTraining/VfLoss             0.167574
-----------------------------  ------------
2025-01-08 09:15:27 | [CL_point_env] epoch #1 | Training Encoder

-----------------------------  ------------
Embedding/MeanContrastiveLoss     0.0578852
PolicyTraining/MeanQ1Vals      -133.569
PolicyTraining/MeanQ2Vals      -133.569
PolicyTraining/MeanVVals       -133.545
PolicyTraining/PolicyLoss       133.639
PolicyTraining/QfLoss             0.800862
PolicyTraining/VfLoss             0.190184
-----------------------------  ------------
-----------------------------  ------------
Embedding/MeanContrastiveLoss     0.0578852
PolicyTraining/MeanQ1Vals      -133.569
PolicyTraining/MeanQ2Vals      -133.569
PolicyTraining/MeanVVals       -133.545
PolicyTraining/PolicyLoss       133.639
PolicyTraining/QfLoss             0.800862
PolicyTraining/VfLoss             0.190184
-----------------------------  ------------
2025-01-08 09:15:47 | [CL_point_env] epoch #1 | Pairwise Similarity Matrix: tensor([[ 1.0000, -0.9999],
        [-0.9999,  1.0000]], grad_fn=<MmBackward0>)
2025-01-08 09:15:47 | [CL_point_env] epoch #1 | Mean Task Embeddings (n_tasks x embedding_dim): tensor([[ 0.2607,  0.6500,  0.0603,  0.2289, -0.6734],
        [-0.2601, -0.6499, -0.0665, -0.2381,  0.6700]],
       grad_fn=<PermuteBackward0>)
-----------------------------  ------------
Embedding/MeanContrastiveLoss     0.0578852
PolicyTraining/MeanQ1Vals      -133.569
PolicyTraining/MeanQ2Vals      -133.569
PolicyTraining/MeanVVals       -133.545
PolicyTraining/PolicyLoss       133.639
PolicyTraining/QfLoss             0.800862
PolicyTraining/VfLoss             0.190184
-----------------------------  ------------
2025-01-08 09:15:47 | [CL_point_env] epoch #1 | Training Encoder

-----------------------------  -----------
Embedding/MeanContrastiveLoss     0.057885
PolicyTraining/MeanQ1Vals      -138.393
PolicyTraining/MeanQ2Vals      -138.393
PolicyTraining/MeanVVals       -138.363
PolicyTraining/PolicyLoss       138.548
PolicyTraining/QfLoss             1.25939
PolicyTraining/VfLoss             0.22313
-----------------------------  -----------
-----------------------------  -----------
Embedding/MeanContrastiveLoss     0.057885
PolicyTraining/MeanQ1Vals      -138.393
PolicyTraining/MeanQ2Vals      -138.393
PolicyTraining/MeanVVals       -138.363
PolicyTraining/PolicyLoss       138.548
PolicyTraining/QfLoss             1.25939
PolicyTraining/VfLoss             0.22313
-----------------------------  -----------
2025-01-08 09:16:08 | [CL_point_env] epoch #1 | Pairwise Similarity Matrix: tensor([[ 1.0000, -0.9999],
        [-0.9999,  1.0000]], grad_fn=<MmBackward0>)
2025-01-08 09:16:08 | [CL_point_env] epoch #1 | Mean Task Embeddings (n_tasks x embedding_dim): tensor([[ 0.2618,  0.6504,  0.0617,  0.2280, -0.6728],
        [-0.2584, -0.6490, -0.0639, -0.2399,  0.6711]],
       grad_fn=<PermuteBackward0>)
-----------------------------  -----------
Embedding/MeanContrastiveLoss     0.057885
PolicyTraining/MeanQ1Vals      -138.393
PolicyTraining/MeanQ2Vals      -138.393
PolicyTraining/MeanVVals       -138.363
PolicyTraining/PolicyLoss       138.548
PolicyTraining/QfLoss             1.25939
PolicyTraining/VfLoss             0.22313
-----------------------------  -----------
2025-01-08 09:16:08 | [CL_point_env] epoch #1 | Training Encoder

-----------------------------  ------------
Embedding/MeanContrastiveLoss     0.0578855
PolicyTraining/MeanQ1Vals      -142.518
PolicyTraining/MeanQ2Vals      -142.518
PolicyTraining/MeanVVals       -142.96
PolicyTraining/PolicyLoss       142.718
PolicyTraining/QfLoss             0.884268
PolicyTraining/VfLoss             0.231165
-----------------------------  ------------
-----------------------------  ------------
Embedding/MeanContrastiveLoss     0.0578855
PolicyTraining/MeanQ1Vals      -142.518
PolicyTraining/MeanQ2Vals      -142.518
PolicyTraining/MeanVVals       -142.96
PolicyTraining/PolicyLoss       142.718
PolicyTraining/QfLoss             0.884268
PolicyTraining/VfLoss             0.231165
-----------------------------  ------------
2025-01-08 09:16:27 | [CL_point_env] epoch #1 | Evaluating...
2025-01-08 09:16:27 | [CL_point_env] epoch #1 | Sampling for adapation and meta-testing...
2025-01-08 09:16:28 | [CL_point_env] epoch #1 | Finished meta-testing...
2025-01-08 09:16:28 | [CL_point_env] epoch #1 | Sampling for adapation and meta-testing...
2025-01-08 09:16:37 | [CL_point_env] epoch #1 | Finished meta-testing...
2025-01-08 09:16:37 | [CL_point_env] epoch #1 | Saving snapshot...
2025-01-08 09:16:37 | [CL_point_env] epoch #1 | Saved
2025-01-08 09:16:37 | [CL_point_env] epoch #1 | Time 650.69 s
2025-01-08 09:16:37 | [CL_point_env] epoch #1 | EpochTime 319.74 s
--------------------------------------------------  -------------
Embedding/MeanContrastiveLoss                           0.0578855
MetaTest/Average/AverageDiscountedReturn             -357.005
MetaTest/Average/AverageReturn                       -357.005
MetaTest/Average/Iteration                              1
MetaTest/Average/MaxReturn                            -24.5149
MetaTest/Average/MinReturn                           -523.507
MetaTest/Average/NumEpisodes                           21
MetaTest/Average/StdReturn                            138.536
MetaTest/Average/SuccessRate                            0.047619
MetaTest/Average/TerminationRate                        0.047619
MetaTest/__unnamed_task__/AverageDiscountedReturn    -357.005
MetaTest/__unnamed_task__/AverageReturn              -357.005
MetaTest/__unnamed_task__/Iteration                     1
MetaTest/__unnamed_task__/MaxReturn                   -24.5149
MetaTest/__unnamed_task__/MinReturn                  -523.507
MetaTest/__unnamed_task__/NumEpisodes                  21
MetaTest/__unnamed_task__/StdReturn                   138.536
MetaTest/__unnamed_task__/SuccessRate                   0.047619
MetaTest/__unnamed_task__/TerminationRate               0.047619
MetaTrain/Average/AverageDiscountedReturn            -186.638
MetaTrain/Average/AverageReturn                      -186.638
MetaTrain/Average/Iteration                             1
MetaTrain/Average/MaxReturn                           -24.9698
MetaTrain/Average/MinReturn                          -524.063
MetaTrain/Average/NumEpisodes                           4
MetaTrain/Average/StdReturn                           203.367
MetaTrain/Average/SuccessRate                           0.5
MetaTrain/Average/TerminationRate                       0.5
MetaTrain/__unnamed_task__/AverageDiscountedReturn   -186.638
MetaTrain/__unnamed_task__/AverageReturn             -186.638
MetaTrain/__unnamed_task__/Iteration                    1
MetaTrain/__unnamed_task__/MaxReturn                  -24.9698
MetaTrain/__unnamed_task__/MinReturn                 -524.063
MetaTrain/__unnamed_task__/NumEpisodes                  4
MetaTrain/__unnamed_task__/StdReturn                  203.367
MetaTrain/__unnamed_task__/SuccessRate                  0.5
MetaTrain/__unnamed_task__/TerminationRate              0.5
PolicyTraining/MeanQ1Vals                            -142.518
PolicyTraining/MeanQ2Vals                            -142.518
PolicyTraining/MeanVVals                             -142.96
PolicyTraining/PolicyLoss                             142.718
PolicyTraining/QfLoss                                   0.884268
PolicyTraining/VfLoss                                   0.231165
TotalEnvSteps                                       11590
--------------------------------------------------  -------------
2025-01-08 09:16:39 | [CL_point_env] epoch #2 | Training...
2025-01-08 09:16:40 | [CL_point_env] epoch #2 | Pairwise Similarity Matrix: tensor([[1.0000, 1.0000],
        [1.0000, 1.0000]], grad_fn=<MmBackward0>)
2025-01-08 09:16:40 | [CL_point_env] epoch #2 | Mean Task Embeddings (n_tasks x embedding_dim): tensor([[ 0.2689,  0.6490,  0.0671,  0.2266, -0.6713],
        [ 0.2720,  0.6495,  0.0698,  0.2276, -0.6690]],
       grad_fn=<PermuteBackward0>)

2025-01-08 09:16:40 | [CL_point_env] epoch #2 | Training Encoder

-----------------------------  -----------
Embedding/MeanContrastiveLoss     0.153897
PolicyTraining/MeanQ1Vals      -152.479
PolicyTraining/MeanQ2Vals      -152.479
PolicyTraining/MeanVVals       -151.197
PolicyTraining/PolicyLoss       151.553
PolicyTraining/QfLoss            15.4137
PolicyTraining/VfLoss             1.52869
-----------------------------  -----------
-----------------------------  -----------
Embedding/MeanContrastiveLoss     0.153897
PolicyTraining/MeanQ1Vals      -152.479
PolicyTraining/MeanQ2Vals      -152.479
PolicyTraining/MeanVVals       -151.197
PolicyTraining/PolicyLoss       151.553
PolicyTraining/QfLoss            15.4137
PolicyTraining/VfLoss             1.52869
-----------------------------  -----------
2025-01-08 09:17:00 | [CL_point_env] epoch #2 | Pairwise Similarity Matrix: tensor([[1.0000, 1.0000],
        [1.0000, 1.0000]], grad_fn=<MmBackward0>)
2025-01-08 09:17:00 | [CL_point_env] epoch #2 | Mean Task Embeddings (n_tasks x embedding_dim): tensor([[ 0.2694,  0.6494,  0.0683,  0.2268, -0.6705],
        [ 0.2717,  0.6500,  0.0710,  0.2277, -0.6684]],
       grad_fn=<PermuteBackward0>)
-----------------------------  -----------
Embedding/MeanContrastiveLoss     0.153897
PolicyTraining/MeanQ1Vals      -152.479
PolicyTraining/MeanQ2Vals      -152.479
PolicyTraining/MeanVVals       -151.197
PolicyTraining/PolicyLoss       151.553
PolicyTraining/QfLoss            15.4137
PolicyTraining/VfLoss             1.52869
-----------------------------  -----------
2025-01-08 09:17:00 | [CL_point_env] epoch #2 | Training Encoder

-----------------------------  -----------
Embedding/MeanContrastiveLoss     0.153897
PolicyTraining/MeanQ1Vals      -157.884
PolicyTraining/MeanQ2Vals      -157.884
PolicyTraining/MeanVVals       -157.273
PolicyTraining/PolicyLoss       157.158
PolicyTraining/QfLoss            14.9067
PolicyTraining/VfLoss             0.480181
-----------------------------  -----------
-----------------------------  -----------
Embedding/MeanContrastiveLoss     0.153897
PolicyTraining/MeanQ1Vals      -157.884
PolicyTraining/MeanQ2Vals      -157.884
PolicyTraining/MeanVVals       -157.273
PolicyTraining/PolicyLoss       157.158
PolicyTraining/QfLoss            14.9067
PolicyTraining/VfLoss             0.480181
-----------------------------  -----------
2025-01-08 09:17:20 | [CL_point_env] epoch #2 | Pairwise Similarity Matrix: tensor([[1.0000, 1.0000],
        [1.0000, 1.0000]], grad_fn=<MmBackward0>)
2025-01-08 09:17:20 | [CL_point_env] epoch #2 | Mean Task Embeddings (n_tasks x embedding_dim): tensor([[ 0.2702,  0.6494,  0.0699,  0.2275, -0.6697],
        [ 0.2717,  0.6506,  0.0716,  0.2284, -0.6675]],
       grad_fn=<PermuteBackward0>)
-----------------------------  -----------
Embedding/MeanContrastiveLoss     0.153897
PolicyTraining/MeanQ1Vals      -157.884
PolicyTraining/MeanQ2Vals      -157.884
PolicyTraining/MeanVVals       -157.273
PolicyTraining/PolicyLoss       157.158
PolicyTraining/QfLoss            14.9067
PolicyTraining/VfLoss             0.480181
-----------------------------  -----------
2025-01-08 09:17:20 | [CL_point_env] epoch #2 | Training Encoder

-----------------------------  -----------
Embedding/MeanContrastiveLoss     0.153896
PolicyTraining/MeanQ1Vals      -161.942
PolicyTraining/MeanQ2Vals      -161.942
PolicyTraining/MeanVVals       -161.579
PolicyTraining/PolicyLoss       161.334
PolicyTraining/QfLoss            13.5286
PolicyTraining/VfLoss             0.522279
-----------------------------  -----------
-----------------------------  -----------
Embedding/MeanContrastiveLoss     0.153896
PolicyTraining/MeanQ1Vals      -161.942
PolicyTraining/MeanQ2Vals      -161.942
PolicyTraining/MeanVVals       -161.579
PolicyTraining/PolicyLoss       161.334
PolicyTraining/QfLoss            13.5286
PolicyTraining/VfLoss             0.522279
-----------------------------  -----------
2025-01-08 09:17:40 | [CL_point_env] epoch #2 | Pairwise Similarity Matrix: tensor([[1.0000, 1.0000],
        [1.0000, 1.0000]], grad_fn=<MmBackward0>)
2025-01-08 09:17:40 | [CL_point_env] epoch #2 | Mean Task Embeddings (n_tasks x embedding_dim): tensor([[ 0.2704,  0.6497,  0.0704,  0.2276, -0.6694],
        [ 0.2733,  0.6503,  0.0737,  0.2287, -0.6668]],
       grad_fn=<PermuteBackward0>)
-----------------------------  -----------
Embedding/MeanContrastiveLoss     0.153896
PolicyTraining/MeanQ1Vals      -161.942
PolicyTraining/MeanQ2Vals      -161.942
PolicyTraining/MeanVVals       -161.579
PolicyTraining/PolicyLoss       161.334
PolicyTraining/QfLoss            13.5286
PolicyTraining/VfLoss             0.522279
-----------------------------  -----------
2025-01-08 09:17:40 | [CL_point_env] epoch #2 | Training Encoder

-----------------------------  -----------
Embedding/MeanContrastiveLoss     0.153896
PolicyTraining/MeanQ1Vals      -169.016
PolicyTraining/MeanQ2Vals      -169.016
PolicyTraining/MeanVVals       -168.583
PolicyTraining/PolicyLoss       168.503
PolicyTraining/QfLoss            13.5428
PolicyTraining/VfLoss             0.404575
-----------------------------  -----------
-----------------------------  -----------
Embedding/MeanContrastiveLoss     0.153896
PolicyTraining/MeanQ1Vals      -169.016
PolicyTraining/MeanQ2Vals      -169.016
PolicyTraining/MeanVVals       -168.583
PolicyTraining/PolicyLoss       168.503
PolicyTraining/QfLoss            13.5428
PolicyTraining/VfLoss             0.404575
-----------------------------  -----------
2025-01-08 09:18:00 | [CL_point_env] epoch #2 | Pairwise Similarity Matrix: tensor([[1.0000, 1.0000],
        [1.0000, 1.0000]], grad_fn=<MmBackward0>)
2025-01-08 09:18:00 | [CL_point_env] epoch #2 | Mean Task Embeddings (n_tasks x embedding_dim): tensor([[ 0.2693,  0.6488,  0.0695,  0.2297, -0.6700],
        [ 0.2724,  0.6499,  0.0729,  0.2309, -0.6670]],
       grad_fn=<PermuteBackward0>)
-----------------------------  -----------
Embedding/MeanContrastiveLoss     0.153896
PolicyTraining/MeanQ1Vals      -169.016
PolicyTraining/MeanQ2Vals      -169.016
PolicyTraining/MeanVVals       -168.583
PolicyTraining/PolicyLoss       168.503
PolicyTraining/QfLoss            13.5428
PolicyTraining/VfLoss             0.404575
-----------------------------  -----------
2025-01-08 09:18:00 | [CL_point_env] epoch #2 | Training Encoder

-----------------------------  -----------
Embedding/MeanContrastiveLoss     0.153896
PolicyTraining/MeanQ1Vals      -170.987
PolicyTraining/MeanQ2Vals      -170.987
PolicyTraining/MeanVVals       -170.384
PolicyTraining/PolicyLoss       170.609
PolicyTraining/QfLoss            12.481
PolicyTraining/VfLoss             0.434759
-----------------------------  -----------
-----------------------------  -----------
Embedding/MeanContrastiveLoss     0.153896
PolicyTraining/MeanQ1Vals      -170.987
PolicyTraining/MeanQ2Vals      -170.987
PolicyTraining/MeanVVals       -170.384
PolicyTraining/PolicyLoss       170.609
PolicyTraining/QfLoss            12.481
PolicyTraining/VfLoss             0.434759
-----------------------------  -----------
2025-01-08 09:18:20 | [CL_point_env] epoch #2 | Pairwise Similarity Matrix: tensor([[1.0000, 1.0000],
        [1.0000, 1.0000]], grad_fn=<MmBackward0>)
2025-01-08 09:18:20 | [CL_point_env] epoch #2 | Mean Task Embeddings (n_tasks x embedding_dim): tensor([[ 0.2702,  0.6495,  0.0709,  0.2288, -0.6692],
        [ 0.2736,  0.6506,  0.0748,  0.2298, -0.6659]],
       grad_fn=<PermuteBackward0>)
-----------------------------  -----------
Embedding/MeanContrastiveLoss     0.153896
PolicyTraining/MeanQ1Vals      -170.987
PolicyTraining/MeanQ2Vals      -170.987
PolicyTraining/MeanVVals       -170.384
PolicyTraining/PolicyLoss       170.609
PolicyTraining/QfLoss            12.481
PolicyTraining/VfLoss             0.434759
-----------------------------  -----------
2025-01-08 09:18:20 | [CL_point_env] epoch #2 | Training Encoder

-----------------------------  -----------
Embedding/MeanContrastiveLoss     0.153896
PolicyTraining/MeanQ1Vals      -180.823
PolicyTraining/MeanQ2Vals      -180.823
PolicyTraining/MeanVVals       -180.488
PolicyTraining/PolicyLoss       180.492
PolicyTraining/QfLoss            12.6139
PolicyTraining/VfLoss             0.453435
-----------------------------  -----------
-----------------------------  -----------
Embedding/MeanContrastiveLoss     0.153896
PolicyTraining/MeanQ1Vals      -180.823
PolicyTraining/MeanQ2Vals      -180.823
PolicyTraining/MeanVVals       -180.488
PolicyTraining/PolicyLoss       180.492
PolicyTraining/QfLoss            12.6139
PolicyTraining/VfLoss             0.453435
-----------------------------  -----------
2025-01-08 09:18:41 | [CL_point_env] epoch #2 | Pairwise Similarity Matrix: tensor([[1.0000, 1.0000],
        [1.0000, 1.0000]], grad_fn=<MmBackward0>)
2025-01-08 09:18:41 | [CL_point_env] epoch #2 | Mean Task Embeddings (n_tasks x embedding_dim): tensor([[ 0.2701,  0.6496,  0.0706,  0.2287, -0.6691],
        [ 0.2732,  0.6506,  0.0747,  0.2298, -0.6661]],
       grad_fn=<PermuteBackward0>)
-----------------------------  -----------
Embedding/MeanContrastiveLoss     0.153896
PolicyTraining/MeanQ1Vals      -180.823
PolicyTraining/MeanQ2Vals      -180.823
PolicyTraining/MeanVVals       -180.488
PolicyTraining/PolicyLoss       180.492
PolicyTraining/QfLoss            12.6139
PolicyTraining/VfLoss             0.453435
-----------------------------  -----------
2025-01-08 09:18:41 | [CL_point_env] epoch #2 | Training Encoder
Traceback (most recent call last):
  File "/Users/paulnitschke/Desktop/projects/geo_meta_rl/garage/examples/torch/multi_env_CL.py", line 197, in <module>
  File "/Users/paulnitschke/Desktop/projects/geo_meta_rl/garage/experiment/experiment.py", line 369, in __call__
    result = self.function(ctxt, **kwargs)
  File "/Users/paulnitschke/Desktop/projects/geo_meta_rl/garage/examples/torch/multi_env_CL.py", line 194, in CL_point_env
    trainer.setup(algo=clmeta, env=env[0]())
  File "/Users/paulnitschke/Desktop/projects/geo_meta_rl/garage/trainer.py", line 396, in train
    average_return = self._algo.train(self)
  File "/Users/paulnitschke/Desktop/projects/geo_meta_rl/garage/torch/algos/CLMeta.py", line 334, in train
    logger.log('Training...')
  File "/Users/paulnitschke/Desktop/projects/geo_meta_rl/garage/torch/algos/CLMeta.py", line 372, in _train_once
    for _ in range(N_STEPS):
  File "/Users/paulnitschke/Desktop/projects/geo_meta_rl/garage/torch/algos/CLMeta.py", line 491, in _optimize_policy
    (q2_pred - q_target)**2)
  File "/Users/paulnitschke/miniconda3/envs/env_geo_meta_rl/lib/python3.9/site-packages/torch/_tensor.py", line 525, in backward
    torch.autograd.backward(
  File "/Users/paulnitschke/miniconda3/envs/env_geo_meta_rl/lib/python3.9/site-packages/torch/autograd/__init__.py", line 267, in backward
    _engine_run_backward(
  File "/Users/paulnitschke/miniconda3/envs/env_geo_meta_rl/lib/python3.9/site-packages/torch/autograd/graph.py", line 744, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
KeyboardInterrupt
Traceback (most recent call last):
  File "/Users/paulnitschke/Desktop/projects/geo_meta_rl/garage/examples/torch/multi_env_CL.py", line 197, in <module>
  File "/Users/paulnitschke/Desktop/projects/geo_meta_rl/garage/experiment/experiment.py", line 369, in __call__
    result = self.function(ctxt, **kwargs)
  File "/Users/paulnitschke/Desktop/projects/geo_meta_rl/garage/examples/torch/multi_env_CL.py", line 194, in CL_point_env
    trainer.setup(algo=clmeta, env=env[0]())
  File "/Users/paulnitschke/Desktop/projects/geo_meta_rl/garage/trainer.py", line 396, in train
    average_return = self._algo.train(self)
  File "/Users/paulnitschke/Desktop/projects/geo_meta_rl/garage/torch/algos/CLMeta.py", line 334, in train
    logger.log('Training...')
  File "/Users/paulnitschke/Desktop/projects/geo_meta_rl/garage/torch/algos/CLMeta.py", line 372, in _train_once
    for _ in range(N_STEPS):
  File "/Users/paulnitschke/Desktop/projects/geo_meta_rl/garage/torch/algos/CLMeta.py", line 491, in _optimize_policy
    (q2_pred - q_target)**2)
  File "/Users/paulnitschke/miniconda3/envs/env_geo_meta_rl/lib/python3.9/site-packages/torch/_tensor.py", line 525, in backward
    torch.autograd.backward(
  File "/Users/paulnitschke/miniconda3/envs/env_geo_meta_rl/lib/python3.9/site-packages/torch/autograd/__init__.py", line 267, in backward
    _engine_run_backward(
  File "/Users/paulnitschke/miniconda3/envs/env_geo_meta_rl/lib/python3.9/site-packages/torch/autograd/graph.py", line 744, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
KeyboardInterrupt

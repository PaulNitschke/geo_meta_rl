2025-01-08 09:39:46 | [CL_point_env] Logging to /Users/paulnitschke/Desktop/projects/geo_meta_rl/data/local/experiment/CL_point_env_9


Failed to import TF-Keras. Please note that TF-Keras is not installed by default when you install TensorFlow Probability. This is so that JAX-only users do not have to install TensorFlow or TF-Keras. To use TensorFlow Probability with TensorFlow, please install the tf-keras or tf-keras-nightly package.
This can be be done through installing the tensorflow-probability[tf] extra.
/Users/paulnitschke/Desktop/projects/geo_meta_rl/garage/experiment/deterministic.py:36: UserWarning: Enabeling deterministic mode in PyTorch can have a performance impact when using GPU.
  warnings.warn(


2025-01-08 09:39:47 | [CL_point_env] Obtaining samples...
/Users/paulnitschke/miniconda3/envs/env_geo_meta_rl/lib/python3.9/site-packages/torch/distributions/distribution.py:53: UserWarning: <class 'garage.torch.distributions.tanh_normal.TanhNormal'> does not define `arg_constraints`. Please set `arg_constraints = {}` or initialize the distribution with `validate_args=False` to turn off validation.
  warnings.warn(
/Users/paulnitschke/Desktop/projects/geo_meta_rl/garage/_dtypes.py:1051: UserWarning: Observation array([0., 0., 2.]) is outside observation_space Box(-inf, inf, (3,), float32)
  warnings.warn(
2025-01-08 09:39:49 | [CL_point_env] epoch #0 | Pre-Training Encoder...
2025-01-08 09:39:50 | [CL_point_env] epoch #0 | Pairwise Similarity Matrix: tensor([[1.0000, 0.9984],
        [0.9984, 1.0000]], grad_fn=<MmBackward0>)
2025-01-08 09:40:07 | [CL_point_env] epoch #0 | Pairwise Similarity Matrix: tensor([[ 1.0000, -0.9989],
        [-0.9989,  1.0000]], grad_fn=<MmBackward0>)
2025-01-08 09:40:07 | [CL_point_env] epoch #0 | Pre-Training Encoder Done...
2025-01-08 09:40:07 | [CL_point_env] epoch #0 | Pairwise Similarity Matrix: tensor([[ 1.0000, -0.9992],
        [-0.9992,  1.0000]], grad_fn=<MmBackward0>)
2025-01-08 09:40:07 | [CL_point_env] epoch #0 | Pairwise Similarity Matrix: tensor([[ 1.0000, -0.9992],
        [-0.9992,  1.0000]], grad_fn=<MmBackward0>)
2025-01-08 09:40:07 | [CL_point_env] epoch #0 | Training...
2025-01-08 09:40:08 | [CL_point_env] epoch #0 | Pairwise Similarity Matrix: tensor([[ 1.0000, -0.9995],
        [-0.9995,  1.0000]], grad_fn=<MmBackward0>)
2025-01-08 09:40:08 | [CL_point_env] epoch #0 | Pairwise Similarity Matrix: tensor([[ 1.0000, -0.9995],
        [-0.9995,  1.0000]], grad_fn=<MmBackward0>)
2025-01-08 09:40:09 | [CL_point_env] epoch #0 | Training Encoder

-----------------------------  -----------
Embedding/MeanContrastiveLoss    0.0579126
PolicyTraining/MeanQ1Vals      -22.432
PolicyTraining/MeanQ2Vals      -22.432
PolicyTraining/MeanVVals        -4.44753
PolicyTraining/PolicyLoss       21.2361
PolicyTraining/QfLoss           13.0887
PolicyTraining/VfLoss          294.253
-----------------------------  -----------
-----------------------------  -----------
Embedding/MeanContrastiveLoss    0.0579126
PolicyTraining/MeanQ1Vals      -22.432
PolicyTraining/MeanQ2Vals      -22.432
PolicyTraining/MeanVVals        -4.44753
PolicyTraining/PolicyLoss       21.2361
PolicyTraining/QfLoss           13.0887
PolicyTraining/VfLoss          294.253
-----------------------------  -----------
2025-01-08 09:40:29 | [CL_point_env] epoch #0 | Pairwise Similarity Matrix: tensor([[ 1.0000, -0.9996],
        [-0.9996,  1.0000]], grad_fn=<MmBackward0>)
2025-01-08 09:40:29 | [CL_point_env] epoch #0 | Pairwise Similarity Matrix: tensor([[ 1.0000, -0.9996],
        [-0.9996,  1.0000]], grad_fn=<MmBackward0>)
2025-01-08 09:40:29 | [CL_point_env] epoch #0 | Training Encoder

-----------------------------  -----------
Embedding/MeanContrastiveLoss    0.0578983
PolicyTraining/MeanQ1Vals      -22.4682
PolicyTraining/MeanQ2Vals      -22.4682
PolicyTraining/MeanVVals       -20.0202
PolicyTraining/PolicyLoss       21.225
PolicyTraining/QfLoss            5.27026
PolicyTraining/VfLoss            2.12084
-----------------------------  -----------
-----------------------------  -----------
Embedding/MeanContrastiveLoss    0.0578983
PolicyTraining/MeanQ1Vals      -22.4682
PolicyTraining/MeanQ2Vals      -22.4682
PolicyTraining/MeanVVals       -20.0202
PolicyTraining/PolicyLoss       21.225
PolicyTraining/QfLoss            5.27026
PolicyTraining/VfLoss            2.12084
-----------------------------  -----------
2025-01-08 09:40:49 | [CL_point_env] epoch #0 | Pairwise Similarity Matrix: tensor([[ 1.0000, -0.9997],
        [-0.9997,  1.0000]], grad_fn=<MmBackward0>)
2025-01-08 09:40:49 | [CL_point_env] epoch #0 | Pairwise Similarity Matrix: tensor([[ 1.0000, -0.9997],
        [-0.9997,  1.0000]], grad_fn=<MmBackward0>)
2025-01-08 09:40:49 | [CL_point_env] epoch #0 | Training Encoder

-----------------------------  -----------
Embedding/MeanContrastiveLoss    0.0579034
PolicyTraining/MeanQ1Vals      -24.3559
PolicyTraining/MeanQ2Vals      -24.3559
PolicyTraining/MeanVVals       -23.0708
PolicyTraining/PolicyLoss       23.1091
PolicyTraining/QfLoss            1.45557
PolicyTraining/VfLoss            0.999558
-----------------------------  -----------
-----------------------------  -----------
Embedding/MeanContrastiveLoss    0.0579034
PolicyTraining/MeanQ1Vals      -24.3559
PolicyTraining/MeanQ2Vals      -24.3559
PolicyTraining/MeanVVals       -23.0708
PolicyTraining/PolicyLoss       23.1091
PolicyTraining/QfLoss            1.45557
PolicyTraining/VfLoss            0.999558
-----------------------------  -----------
2025-01-08 09:41:09 | [CL_point_env] epoch #0 | Pairwise Similarity Matrix: tensor([[ 1.0000, -0.9999],
        [-0.9999,  1.0000]], grad_fn=<MmBackward0>)
2025-01-08 09:41:09 | [CL_point_env] epoch #0 | Pairwise Similarity Matrix: tensor([[ 1.0000, -0.9999],
        [-0.9999,  1.0000]], grad_fn=<MmBackward0>)
2025-01-08 09:41:09 | [CL_point_env] epoch #0 | Training Encoder

-----------------------------  -----------
Embedding/MeanContrastiveLoss    0.0578935
PolicyTraining/MeanQ1Vals      -26.7977
PolicyTraining/MeanQ2Vals      -26.7977
PolicyTraining/MeanVVals       -25.6567
PolicyTraining/PolicyLoss       25.6114
PolicyTraining/QfLoss            0.485083
PolicyTraining/VfLoss            0.89481
-----------------------------  -----------
-----------------------------  -----------
Embedding/MeanContrastiveLoss    0.0578935
PolicyTraining/MeanQ1Vals      -26.7977
PolicyTraining/MeanQ2Vals      -26.7977
PolicyTraining/MeanVVals       -25.6567
PolicyTraining/PolicyLoss       25.6114
PolicyTraining/QfLoss            0.485083
PolicyTraining/VfLoss            0.89481
-----------------------------  -----------
2025-01-08 09:41:29 | [CL_point_env] epoch #0 | Pairwise Similarity Matrix: tensor([[ 1.0000, -0.9998],
        [-0.9998,  1.0000]], grad_fn=<MmBackward0>)
2025-01-08 09:41:29 | [CL_point_env] epoch #0 | Pairwise Similarity Matrix: tensor([[ 1.0000, -0.9998],
        [-0.9998,  1.0000]], grad_fn=<MmBackward0>)
2025-01-08 09:41:29 | [CL_point_env] epoch #0 | Training Encoder

-----------------------------  -----------
Embedding/MeanContrastiveLoss    0.0578926
PolicyTraining/MeanQ1Vals      -30.7752
PolicyTraining/MeanQ2Vals      -30.7752
PolicyTraining/MeanVVals       -29.6161
PolicyTraining/PolicyLoss       29.6354
PolicyTraining/QfLoss            0.295163
PolicyTraining/VfLoss            0.113008
-----------------------------  -----------
-----------------------------  -----------
Embedding/MeanContrastiveLoss    0.0578926
PolicyTraining/MeanQ1Vals      -30.7752
PolicyTraining/MeanQ2Vals      -30.7752
PolicyTraining/MeanVVals       -29.6161
PolicyTraining/PolicyLoss       29.6354
PolicyTraining/QfLoss            0.295163
PolicyTraining/VfLoss            0.113008
-----------------------------  -----------
2025-01-08 09:41:48 | [CL_point_env] epoch #0 | Pairwise Similarity Matrix: tensor([[ 1.0000, -0.9999],
        [-0.9999,  1.0000]], grad_fn=<MmBackward0>)
2025-01-08 09:41:48 | [CL_point_env] epoch #0 | Pairwise Similarity Matrix: tensor([[ 1.0000, -0.9999],
        [-0.9999,  1.0000]], grad_fn=<MmBackward0>)
2025-01-08 09:41:48 | [CL_point_env] epoch #0 | Training Encoder

-----------------------------  -----------
Embedding/MeanContrastiveLoss    0.0578928
PolicyTraining/MeanQ1Vals      -33.8309
PolicyTraining/MeanQ2Vals      -33.8309
PolicyTraining/MeanVVals       -32.7075
PolicyTraining/PolicyLoss       32.7265
PolicyTraining/QfLoss            0.208861
PolicyTraining/VfLoss            0.0758786
-----------------------------  -----------
-----------------------------  -----------
Embedding/MeanContrastiveLoss    0.0578928
PolicyTraining/MeanQ1Vals      -33.8309
PolicyTraining/MeanQ2Vals      -33.8309
PolicyTraining/MeanVVals       -32.7075
PolicyTraining/PolicyLoss       32.7265
PolicyTraining/QfLoss            0.208861
PolicyTraining/VfLoss            0.0758786
-----------------------------  -----------
2025-01-08 09:42:08 | [CL_point_env] epoch #0 | Pairwise Similarity Matrix: tensor([[ 1.0000, -1.0000],
        [-1.0000,  1.0000]], grad_fn=<MmBackward0>)
2025-01-08 09:42:08 | [CL_point_env] epoch #0 | Pairwise Similarity Matrix: tensor([[ 1.0000, -1.0000],
        [-1.0000,  1.0000]], grad_fn=<MmBackward0>)
2025-01-08 09:42:08 | [CL_point_env] epoch #0 | Training Encoder

-----------------------------  -----------
Embedding/MeanContrastiveLoss    0.0578884
PolicyTraining/MeanQ1Vals      -34.8297
PolicyTraining/MeanQ2Vals      -34.8297
PolicyTraining/MeanVVals       -33.7868
PolicyTraining/PolicyLoss       33.7824
PolicyTraining/QfLoss            0.136801
PolicyTraining/VfLoss            0.0723519
-----------------------------  -----------
-----------------------------  -----------
Embedding/MeanContrastiveLoss    0.0578884
PolicyTraining/MeanQ1Vals      -34.8297
PolicyTraining/MeanQ2Vals      -34.8297
PolicyTraining/MeanVVals       -33.7868
PolicyTraining/PolicyLoss       33.7824
PolicyTraining/QfLoss            0.136801
PolicyTraining/VfLoss            0.0723519
-----------------------------  -----------
2025-01-08 09:42:28 | [CL_point_env] epoch #0 | Pairwise Similarity Matrix: tensor([[ 1.0000, -0.9999],
        [-0.9999,  1.0000]], grad_fn=<MmBackward0>)
2025-01-08 09:42:28 | [CL_point_env] epoch #0 | Pairwise Similarity Matrix: tensor([[ 1.0000, -0.9999],
        [-0.9999,  1.0000]], grad_fn=<MmBackward0>)
2025-01-08 09:42:28 | [CL_point_env] epoch #0 | Training Encoder

-----------------------------  -----------
Embedding/MeanContrastiveLoss    0.0578893
PolicyTraining/MeanQ1Vals      -39.3191
PolicyTraining/MeanQ2Vals      -39.3191
PolicyTraining/MeanVVals       -38.3398
PolicyTraining/PolicyLoss       38.3496
PolicyTraining/QfLoss            0.105255
PolicyTraining/VfLoss            0.0640955
-----------------------------  -----------
-----------------------------  -----------
Embedding/MeanContrastiveLoss    0.0578893
PolicyTraining/MeanQ1Vals      -39.3191
PolicyTraining/MeanQ2Vals      -39.3191
PolicyTraining/MeanVVals       -38.3398
PolicyTraining/PolicyLoss       38.3496
PolicyTraining/QfLoss            0.105255
PolicyTraining/VfLoss            0.0640955
-----------------------------  -----------
2025-01-08 09:42:48 | [CL_point_env] epoch #0 | Pairwise Similarity Matrix: tensor([[ 1.0000, -1.0000],
        [-1.0000,  1.0000]], grad_fn=<MmBackward0>)
2025-01-08 09:42:48 | [CL_point_env] epoch #0 | Pairwise Similarity Matrix: tensor([[ 1.0000, -1.0000],
        [-1.0000,  1.0000]], grad_fn=<MmBackward0>)
2025-01-08 09:42:48 | [CL_point_env] epoch #0 | Training Encoder

-----------------------------  -----------
Embedding/MeanContrastiveLoss    0.0578889
PolicyTraining/MeanQ1Vals      -41.9295
PolicyTraining/MeanQ2Vals      -41.9295
PolicyTraining/MeanVVals       -41.0283
PolicyTraining/PolicyLoss       41.0142
PolicyTraining/QfLoss            0.10244
PolicyTraining/VfLoss            0.0648677
-----------------------------  -----------
-----------------------------  -----------
Embedding/MeanContrastiveLoss    0.0578889
PolicyTraining/MeanQ1Vals      -41.9295
PolicyTraining/MeanQ2Vals      -41.9295
PolicyTraining/MeanVVals       -41.0283
PolicyTraining/PolicyLoss       41.0142
PolicyTraining/QfLoss            0.10244
PolicyTraining/VfLoss            0.0648677
-----------------------------  -----------
2025-01-08 09:43:08 | [CL_point_env] epoch #0 | Pairwise Similarity Matrix: tensor([[ 1.0000, -1.0000],
        [-1.0000,  1.0000]], grad_fn=<MmBackward0>)
2025-01-08 09:43:08 | [CL_point_env] epoch #0 | Pairwise Similarity Matrix: tensor([[ 1.0000, -1.0000],
        [-1.0000,  1.0000]], grad_fn=<MmBackward0>)
2025-01-08 09:43:08 | [CL_point_env] epoch #0 | Training Encoder

-----------------------------  -----------
Embedding/MeanContrastiveLoss    0.0578864
PolicyTraining/MeanQ1Vals      -45.1105
PolicyTraining/MeanQ2Vals      -45.1105
PolicyTraining/MeanVVals       -44.2157
PolicyTraining/PolicyLoss       44.2664
PolicyTraining/QfLoss            0.0868079
PolicyTraining/VfLoss            0.0710359
-----------------------------  -----------
-----------------------------  -----------
Embedding/MeanContrastiveLoss    0.0578864
PolicyTraining/MeanQ1Vals      -45.1105
PolicyTraining/MeanQ2Vals      -45.1105
PolicyTraining/MeanVVals       -44.2157
PolicyTraining/PolicyLoss       44.2664
PolicyTraining/QfLoss            0.0868079
PolicyTraining/VfLoss            0.0710359
-----------------------------  -----------
2025-01-08 09:43:27 | [CL_point_env] epoch #0 | Pairwise Similarity Matrix: tensor([[ 1.0000, -1.0000],
        [-1.0000,  1.0000]], grad_fn=<MmBackward0>)
2025-01-08 09:43:27 | [CL_point_env] epoch #0 | Pairwise Similarity Matrix: tensor([[ 1.0000, -1.0000],
        [-1.0000,  1.0000]], grad_fn=<MmBackward0>)
2025-01-08 09:43:27 | [CL_point_env] epoch #0 | Training Encoder

-----------------------------  -----------
Embedding/MeanContrastiveLoss    0.0578875
PolicyTraining/MeanQ1Vals      -51.35
PolicyTraining/MeanQ2Vals      -51.35
PolicyTraining/MeanVVals       -50.6032
PolicyTraining/PolicyLoss       50.6139
PolicyTraining/QfLoss            0.0782334
PolicyTraining/VfLoss            0.0590598
-----------------------------  -----------
-----------------------------  -----------
Embedding/MeanContrastiveLoss    0.0578875
PolicyTraining/MeanQ1Vals      -51.35
PolicyTraining/MeanQ2Vals      -51.35
PolicyTraining/MeanVVals       -50.6032
PolicyTraining/PolicyLoss       50.6139
PolicyTraining/QfLoss            0.0782334
PolicyTraining/VfLoss            0.0590598
-----------------------------  -----------
2025-01-08 09:43:47 | [CL_point_env] epoch #0 | Pairwise Similarity Matrix: tensor([[ 1.0000, -1.0000],
        [-1.0000,  1.0000]], grad_fn=<MmBackward0>)
2025-01-08 09:43:47 | [CL_point_env] epoch #0 | Pairwise Similarity Matrix: tensor([[ 1.0000, -1.0000],
        [-1.0000,  1.0000]], grad_fn=<MmBackward0>)
2025-01-08 09:43:47 | [CL_point_env] epoch #0 | Training Encoder

-----------------------------  -----------
Embedding/MeanContrastiveLoss    0.0578864
PolicyTraining/MeanQ1Vals      -55.1313
PolicyTraining/MeanQ2Vals      -55.1313
PolicyTraining/MeanVVals       -54.4172
PolicyTraining/PolicyLoss       54.4564
PolicyTraining/QfLoss            0.0725208
PolicyTraining/VfLoss            0.069953
-----------------------------  -----------
-----------------------------  -----------
Embedding/MeanContrastiveLoss    0.0578864
PolicyTraining/MeanQ1Vals      -55.1313
PolicyTraining/MeanQ2Vals      -55.1313
PolicyTraining/MeanVVals       -54.4172
PolicyTraining/PolicyLoss       54.4564
PolicyTraining/QfLoss            0.0725208
PolicyTraining/VfLoss            0.069953
-----------------------------  -----------
2025-01-08 09:44:07 | [CL_point_env] epoch #0 | Pairwise Similarity Matrix: tensor([[ 1.0000, -1.0000],
        [-1.0000,  1.0000]], grad_fn=<MmBackward0>)
2025-01-08 09:44:07 | [CL_point_env] epoch #0 | Pairwise Similarity Matrix: tensor([[ 1.0000, -1.0000],
        [-1.0000,  1.0000]], grad_fn=<MmBackward0>)
2025-01-08 09:44:07 | [CL_point_env] epoch #0 | Training Encoder

-----------------------------  -----------
Embedding/MeanContrastiveLoss    0.0578858
PolicyTraining/MeanQ1Vals      -56.8077
PolicyTraining/MeanQ2Vals      -56.8077
PolicyTraining/MeanVVals       -56.3087
PolicyTraining/PolicyLoss       56.2309
PolicyTraining/QfLoss            0.0737983
PolicyTraining/VfLoss            0.0646639
-----------------------------  -----------
-----------------------------  -----------
Embedding/MeanContrastiveLoss    0.0578858
PolicyTraining/MeanQ1Vals      -56.8077
PolicyTraining/MeanQ2Vals      -56.8077
PolicyTraining/MeanVVals       -56.3087
PolicyTraining/PolicyLoss       56.2309
PolicyTraining/QfLoss            0.0737983
PolicyTraining/VfLoss            0.0646639
-----------------------------  -----------
2025-01-08 09:44:27 | [CL_point_env] epoch #0 | Pairwise Similarity Matrix: tensor([[ 1.0000, -1.0000],
        [-1.0000,  1.0000]], grad_fn=<MmBackward0>)
2025-01-08 09:44:27 | [CL_point_env] epoch #0 | Pairwise Similarity Matrix: tensor([[ 1.0000, -1.0000],
        [-1.0000,  1.0000]], grad_fn=<MmBackward0>)
2025-01-08 09:44:27 | [CL_point_env] epoch #0 | Training Encoder

-----------------------------  -----------
Embedding/MeanContrastiveLoss    0.0578864
PolicyTraining/MeanQ1Vals      -59.0754
PolicyTraining/MeanQ2Vals      -59.0754
PolicyTraining/MeanVVals       -58.5411
PolicyTraining/PolicyLoss       58.5598
PolicyTraining/QfLoss            0.0779131
PolicyTraining/VfLoss            0.0692206
-----------------------------  -----------
-----------------------------  -----------
Embedding/MeanContrastiveLoss    0.0578864
PolicyTraining/MeanQ1Vals      -59.0754
PolicyTraining/MeanQ2Vals      -59.0754
PolicyTraining/MeanVVals       -58.5411
PolicyTraining/PolicyLoss       58.5598
PolicyTraining/QfLoss            0.0779131
PolicyTraining/VfLoss            0.0692206
-----------------------------  -----------
2025-01-08 09:44:47 | [CL_point_env] epoch #0 | Pairwise Similarity Matrix: tensor([[ 1.0000, -1.0000],
        [-1.0000,  1.0000]], grad_fn=<MmBackward0>)
2025-01-08 09:44:47 | [CL_point_env] epoch #0 | Pairwise Similarity Matrix: tensor([[ 1.0000, -1.0000],
        [-1.0000,  1.0000]], grad_fn=<MmBackward0>)
2025-01-08 09:44:47 | [CL_point_env] epoch #0 | Training Encoder

-----------------------------  -----------
Embedding/MeanContrastiveLoss    0.0578852
PolicyTraining/MeanQ1Vals      -62.6757
PolicyTraining/MeanQ2Vals      -62.6757
PolicyTraining/MeanVVals       -62.2326
PolicyTraining/PolicyLoss       62.2317
PolicyTraining/QfLoss            0.0677262
PolicyTraining/VfLoss            0.0717044
-----------------------------  -----------
-----------------------------  -----------
Embedding/MeanContrastiveLoss    0.0578852
PolicyTraining/MeanQ1Vals      -62.6757
PolicyTraining/MeanQ2Vals      -62.6757
PolicyTraining/MeanVVals       -62.2326
PolicyTraining/PolicyLoss       62.2317
PolicyTraining/QfLoss            0.0677262
PolicyTraining/VfLoss            0.0717044
-----------------------------  -----------
2025-01-08 09:45:06 | [CL_point_env] epoch #0 | Evaluating...
2025-01-08 09:45:06 | [CL_point_env] epoch #0 | Sampling for adapation and meta-testing...


Failed to import TF-Keras. Please note that TF-Keras is not installed by default when you install TensorFlow Probability. This is so that JAX-only users do not have to install TensorFlow or TF-Keras. To use TensorFlow Probability with TensorFlow, please install the tf-keras or tf-keras-nightly package.
This can be be done through installing the tensorflow-probability[tf] extra.
/Users/paulnitschke/Desktop/projects/geo_meta_rl/garage/experiment/deterministic.py:36: UserWarning: Enabeling deterministic mode in PyTorch can have a performance impact when using GPU.
  warnings.warn(


2025-01-08 09:45:07 | [CL_point_env] epoch #0 | Finished meta-testing...
2025-01-08 09:45:07 | [CL_point_env] epoch #0 | Sampling for adapation and meta-testing...


Failed to import TF-Keras. Please note that TF-Keras is not installed by default when you install TensorFlow Probability. This is so that JAX-only users do not have to install TensorFlow or TF-Keras. To use TensorFlow Probability with TensorFlow, please install the tf-keras or tf-keras-nightly package.
This can be be done through installing the tensorflow-probability[tf] extra.


2025-01-08 09:45:15 | [CL_point_env] epoch #0 | Finished meta-testing...
2025-01-08 09:45:15 | [CL_point_env] epoch #0 | Saving snapshot...
2025-01-08 09:45:15 | [CL_point_env] epoch #0 | Saved
2025-01-08 09:45:15 | [CL_point_env] epoch #0 | Time 328.47 s
2025-01-08 09:45:15 | [CL_point_env] epoch #0 | EpochTime 328.47 s
--------------------------------------------------  ------------
Embedding/MeanContrastiveLoss                          0.0578852
MetaTest/Average/AverageDiscountedReturn            -480.349
MetaTest/Average/AverageReturn                      -480.349
MetaTest/Average/Iteration                             0
MetaTest/Average/MaxReturn                          -286.006
MetaTest/Average/MinReturn                          -563.339
MetaTest/Average/NumEpisodes                          20
MetaTest/Average/StdReturn                            83.5783
MetaTest/Average/SuccessRate                           0
MetaTest/Average/TerminationRate                       0
MetaTest/__unnamed_task__/AverageDiscountedReturn   -480.349
MetaTest/__unnamed_task__/AverageReturn             -480.349
MetaTest/__unnamed_task__/Iteration                    0
MetaTest/__unnamed_task__/MaxReturn                 -286.006
MetaTest/__unnamed_task__/MinReturn                 -563.339
MetaTest/__unnamed_task__/NumEpisodes                 20
MetaTest/__unnamed_task__/StdReturn                   83.5783
MetaTest/__unnamed_task__/SuccessRate                  0
MetaTest/__unnamed_task__/TerminationRate              0
MetaTrain/Average/AverageDiscountedReturn           -557.496
MetaTrain/Average/AverageReturn                     -557.496
MetaTrain/Average/Iteration                            0
MetaTrain/Average/MaxReturn                         -551.652
MetaTrain/Average/MinReturn                         -563.339
MetaTrain/Average/NumEpisodes                          2
MetaTrain/Average/StdReturn                            5.84347
MetaTrain/Average/SuccessRate                          0
MetaTrain/Average/TerminationRate                      0
MetaTrain/__unnamed_task__/AverageDiscountedReturn  -557.496
MetaTrain/__unnamed_task__/AverageReturn            -557.496
MetaTrain/__unnamed_task__/Iteration                   0
MetaTrain/__unnamed_task__/MaxReturn                -551.652
MetaTrain/__unnamed_task__/MinReturn                -563.339
MetaTrain/__unnamed_task__/NumEpisodes                 2
MetaTrain/__unnamed_task__/StdReturn                   5.84347
MetaTrain/__unnamed_task__/SuccessRate                 0
MetaTrain/__unnamed_task__/TerminationRate             0
PolicyTraining/MeanQ1Vals                            -62.6757
PolicyTraining/MeanQ2Vals                            -62.6757
PolicyTraining/MeanVVals                             -62.2326
PolicyTraining/PolicyLoss                             62.2317
PolicyTraining/QfLoss                                  0.0677262
PolicyTraining/VfLoss                                  0.0717044
TotalEnvSteps                                       6900
--------------------------------------------------  ------------
2025-01-08 09:45:17 | [CL_point_env] epoch #1 | Training...
2025-01-08 09:45:21 | [CL_point_env] epoch #1 | Pairwise Similarity Matrix: tensor([[ 1.0000, -0.9910],
        [-0.9910,  1.0000]], grad_fn=<MmBackward0>)
2025-01-08 09:45:21 | [CL_point_env] epoch #1 | Pairwise Similarity Matrix: tensor([[ 1.0000, -0.9910],
        [-0.9910,  1.0000]], grad_fn=<MmBackward0>)
2025-01-08 09:45:21 | [CL_point_env] epoch #1 | Training Encoder

-----------------------------  ----------
Embedding/MeanContrastiveLoss    0.226217
PolicyTraining/MeanQ1Vals      -79.6444
PolicyTraining/MeanQ2Vals      -79.6444
PolicyTraining/MeanVVals       -78.837
PolicyTraining/PolicyLoss       78.8873
PolicyTraining/QfLoss            0.992861
PolicyTraining/VfLoss            0.309821
-----------------------------  ----------
-----------------------------  ----------
Embedding/MeanContrastiveLoss    0.226217
PolicyTraining/MeanQ1Vals      -79.6444
PolicyTraining/MeanQ2Vals      -79.6444
PolicyTraining/MeanVVals       -78.837
PolicyTraining/PolicyLoss       78.8873
PolicyTraining/QfLoss            0.992861
PolicyTraining/VfLoss            0.309821
-----------------------------  ----------
2025-01-08 09:45:42 | [CL_point_env] epoch #1 | Pairwise Similarity Matrix: tensor([[ 1.0000, -0.9725],
        [-0.9725,  1.0000]], grad_fn=<MmBackward0>)
2025-01-08 09:45:42 | [CL_point_env] epoch #1 | Pairwise Similarity Matrix: tensor([[ 1.0000, -0.9725],
        [-0.9725,  1.0000]], grad_fn=<MmBackward0>)
2025-01-08 09:45:42 | [CL_point_env] epoch #1 | Training Encoder

-----------------------------  ----------
Embedding/MeanContrastiveLoss    0.117733
PolicyTraining/MeanQ1Vals      -84.71
PolicyTraining/MeanQ2Vals      -84.71
PolicyTraining/MeanVVals       -84.007
PolicyTraining/PolicyLoss       84.0439
PolicyTraining/QfLoss            1.10192
PolicyTraining/VfLoss            0.158689
-----------------------------  ----------
-----------------------------  ----------
Embedding/MeanContrastiveLoss    0.117733
PolicyTraining/MeanQ1Vals      -84.71
PolicyTraining/MeanQ2Vals      -84.71
PolicyTraining/MeanVVals       -84.007
PolicyTraining/PolicyLoss       84.0439
PolicyTraining/QfLoss            1.10192
PolicyTraining/VfLoss            0.158689
-----------------------------  ----------
2025-01-08 09:46:03 | [CL_point_env] epoch #1 | Pairwise Similarity Matrix: tensor([[ 1.0000, -0.9988],
        [-0.9988,  1.0000]], grad_fn=<MmBackward0>)
2025-01-08 09:46:03 | [CL_point_env] epoch #1 | Pairwise Similarity Matrix: tensor([[ 1.0000, -0.9988],
        [-0.9988,  1.0000]], grad_fn=<MmBackward0>)
2025-01-08 09:46:03 | [CL_point_env] epoch #1 | Training Encoder

-----------------------------  ----------
Embedding/MeanContrastiveLoss    0.135813
PolicyTraining/MeanQ1Vals      -88.6458
PolicyTraining/MeanQ2Vals      -88.6458
PolicyTraining/MeanVVals       -87.9817
PolicyTraining/PolicyLoss       88.0424
PolicyTraining/QfLoss            1.6106
PolicyTraining/VfLoss            0.166872
-----------------------------  ----------
-----------------------------  ----------
Embedding/MeanContrastiveLoss    0.135813
PolicyTraining/MeanQ1Vals      -88.6458
PolicyTraining/MeanQ2Vals      -88.6458
PolicyTraining/MeanVVals       -87.9817
PolicyTraining/PolicyLoss       88.0424
PolicyTraining/QfLoss            1.6106
PolicyTraining/VfLoss            0.166872
-----------------------------  ----------
2025-01-08 09:46:23 | [CL_point_env] epoch #1 | Pairwise Similarity Matrix: tensor([[ 1.0000, -0.9963],
        [-0.9963,  1.0000]], grad_fn=<MmBackward0>)
2025-01-08 09:46:23 | [CL_point_env] epoch #1 | Pairwise Similarity Matrix: tensor([[ 1.0000, -0.9963],
        [-0.9963,  1.0000]], grad_fn=<MmBackward0>)
2025-01-08 09:46:23 | [CL_point_env] epoch #1 | Training Encoder

-----------------------------  -----------
Embedding/MeanContrastiveLoss    0.0579877
PolicyTraining/MeanQ1Vals      -94.1924
PolicyTraining/MeanQ2Vals      -94.1924
PolicyTraining/MeanVVals       -93.6821
PolicyTraining/PolicyLoss       93.6607
PolicyTraining/QfLoss            0.645092
PolicyTraining/VfLoss            0.140479
-----------------------------  -----------
-----------------------------  -----------
Embedding/MeanContrastiveLoss    0.0579877
PolicyTraining/MeanQ1Vals      -94.1924
PolicyTraining/MeanQ2Vals      -94.1924
PolicyTraining/MeanVVals       -93.6821
PolicyTraining/PolicyLoss       93.6607
PolicyTraining/QfLoss            0.645092
PolicyTraining/VfLoss            0.140479
-----------------------------  -----------
2025-01-08 09:46:43 | [CL_point_env] epoch #1 | Pairwise Similarity Matrix: tensor([[ 1.0000, -0.9957],
        [-0.9957,  1.0000]], grad_fn=<MmBackward0>)
2025-01-08 09:46:43 | [CL_point_env] epoch #1 | Pairwise Similarity Matrix: tensor([[ 1.0000, -0.9957],
        [-0.9957,  1.0000]], grad_fn=<MmBackward0>)
2025-01-08 09:46:43 | [CL_point_env] epoch #1 | Training Encoder

-----------------------------  -----------
Embedding/MeanContrastiveLoss    0.0580483
PolicyTraining/MeanQ1Vals      -97.008
PolicyTraining/MeanQ2Vals      -97.008
PolicyTraining/MeanVVals       -96.4804
PolicyTraining/PolicyLoss       96.5384
PolicyTraining/QfLoss            0.605204
PolicyTraining/VfLoss            0.143258
-----------------------------  -----------
-----------------------------  -----------
Embedding/MeanContrastiveLoss    0.0580483
PolicyTraining/MeanQ1Vals      -97.008
PolicyTraining/MeanQ2Vals      -97.008
PolicyTraining/MeanVVals       -96.4804
PolicyTraining/PolicyLoss       96.5384
PolicyTraining/QfLoss            0.605204
PolicyTraining/VfLoss            0.143258
-----------------------------  -----------
2025-01-08 09:47:03 | [CL_point_env] epoch #1 | Pairwise Similarity Matrix: tensor([[ 1.0000, -0.9978],
        [-0.9978,  1.0000]], grad_fn=<MmBackward0>)
2025-01-08 09:47:03 | [CL_point_env] epoch #1 | Pairwise Similarity Matrix: tensor([[ 1.0000, -0.9978],
        [-0.9978,  1.0000]], grad_fn=<MmBackward0>)
2025-01-08 09:47:03 | [CL_point_env] epoch #1 | Training Encoder

-----------------------------  -----------
Embedding/MeanContrastiveLoss     0.057995
PolicyTraining/MeanQ1Vals      -103.593
PolicyTraining/MeanQ2Vals      -103.593
PolicyTraining/MeanVVals       -103.338
PolicyTraining/PolicyLoss       103.207
PolicyTraining/QfLoss             0.904506
PolicyTraining/VfLoss             0.157135
-----------------------------  -----------
-----------------------------  -----------
Embedding/MeanContrastiveLoss     0.057995
PolicyTraining/MeanQ1Vals      -103.593
PolicyTraining/MeanQ2Vals      -103.593
PolicyTraining/MeanVVals       -103.338
PolicyTraining/PolicyLoss       103.207
PolicyTraining/QfLoss             0.904506
PolicyTraining/VfLoss             0.157135
-----------------------------  -----------
2025-01-08 09:47:23 | [CL_point_env] epoch #1 | Pairwise Similarity Matrix: tensor([[ 1.0000, -0.9994],
        [-0.9994,  1.0000]], grad_fn=<MmBackward0>)
2025-01-08 09:47:23 | [CL_point_env] epoch #1 | Pairwise Similarity Matrix: tensor([[ 1.0000, -0.9994],
        [-0.9994,  1.0000]], grad_fn=<MmBackward0>)
2025-01-08 09:47:23 | [CL_point_env] epoch #1 | Training Encoder

-----------------------------  ------------
Embedding/MeanContrastiveLoss     0.0579231
PolicyTraining/MeanQ1Vals      -107.724
PolicyTraining/MeanQ2Vals      -107.724
PolicyTraining/MeanVVals       -107.601
PolicyTraining/PolicyLoss       107.448
PolicyTraining/QfLoss             0.769208
PolicyTraining/VfLoss             0.158006
-----------------------------  ------------
-----------------------------  ------------
Embedding/MeanContrastiveLoss     0.0579231
PolicyTraining/MeanQ1Vals      -107.724
PolicyTraining/MeanQ2Vals      -107.724
PolicyTraining/MeanVVals       -107.601
PolicyTraining/PolicyLoss       107.448
PolicyTraining/QfLoss             0.769208
PolicyTraining/VfLoss             0.158006
-----------------------------  ------------
2025-01-08 09:47:43 | [CL_point_env] epoch #1 | Pairwise Similarity Matrix: tensor([[ 1.0000, -0.9996],
        [-0.9996,  1.0000]], grad_fn=<MmBackward0>)
2025-01-08 09:47:43 | [CL_point_env] epoch #1 | Pairwise Similarity Matrix: tensor([[ 1.0000, -0.9996],
        [-0.9996,  1.0000]], grad_fn=<MmBackward0>)
2025-01-08 09:47:43 | [CL_point_env] epoch #1 | Training Encoder

-----------------------------  ------------
Embedding/MeanContrastiveLoss     0.0578989
PolicyTraining/MeanQ1Vals      -111.716
PolicyTraining/MeanQ2Vals      -111.716
PolicyTraining/MeanVVals       -111.517
PolicyTraining/PolicyLoss       111.458
PolicyTraining/QfLoss             0.915689
PolicyTraining/VfLoss             0.14947
-----------------------------  ------------
-----------------------------  ------------
Embedding/MeanContrastiveLoss     0.0578989
PolicyTraining/MeanQ1Vals      -111.716
PolicyTraining/MeanQ2Vals      -111.716
PolicyTraining/MeanVVals       -111.517
PolicyTraining/PolicyLoss       111.458
PolicyTraining/QfLoss             0.915689
PolicyTraining/VfLoss             0.14947
-----------------------------  ------------
2025-01-08 09:48:03 | [CL_point_env] epoch #1 | Pairwise Similarity Matrix: tensor([[ 1.0000, -0.9995],
        [-0.9995,  1.0000]], grad_fn=<MmBackward0>)
2025-01-08 09:48:03 | [CL_point_env] epoch #1 | Pairwise Similarity Matrix: tensor([[ 1.0000, -0.9995],
        [-0.9995,  1.0000]], grad_fn=<MmBackward0>)
2025-01-08 09:48:03 | [CL_point_env] epoch #1 | Training Encoder

-----------------------------  -----------
Embedding/MeanContrastiveLoss     0.057903
PolicyTraining/MeanQ1Vals      -116.081
PolicyTraining/MeanQ2Vals      -116.081
PolicyTraining/MeanVVals       -115.771
PolicyTraining/PolicyLoss       115.837
PolicyTraining/QfLoss             0.71346
PolicyTraining/VfLoss             0.159768
-----------------------------  -----------
-----------------------------  -----------
Embedding/MeanContrastiveLoss     0.057903
PolicyTraining/MeanQ1Vals      -116.081
PolicyTraining/MeanQ2Vals      -116.081
PolicyTraining/MeanVVals       -115.771
PolicyTraining/PolicyLoss       115.837
PolicyTraining/QfLoss             0.71346
PolicyTraining/VfLoss             0.159768
-----------------------------  -----------
2025-01-08 09:48:23 | [CL_point_env] epoch #1 | Pairwise Similarity Matrix: tensor([[ 1.0000, -0.9995],
        [-0.9995,  1.0000]], grad_fn=<MmBackward0>)
2025-01-08 09:48:23 | [CL_point_env] epoch #1 | Pairwise Similarity Matrix: tensor([[ 1.0000, -0.9995],
        [-0.9995,  1.0000]], grad_fn=<MmBackward0>)
2025-01-08 09:48:23 | [CL_point_env] epoch #1 | Training Encoder

-----------------------------  ------------
Embedding/MeanContrastiveLoss     0.0579028
PolicyTraining/MeanQ1Vals      -121.041
PolicyTraining/MeanQ2Vals      -121.041
PolicyTraining/MeanVVals       -120.952
PolicyTraining/PolicyLoss       120.933
PolicyTraining/QfLoss             0.843346
PolicyTraining/VfLoss             0.179578
-----------------------------  ------------
-----------------------------  ------------
Embedding/MeanContrastiveLoss     0.0579028
PolicyTraining/MeanQ1Vals      -121.041
PolicyTraining/MeanQ2Vals      -121.041
PolicyTraining/MeanVVals       -120.952
PolicyTraining/PolicyLoss       120.933
PolicyTraining/QfLoss             0.843346
PolicyTraining/VfLoss             0.179578
-----------------------------  ------------
2025-01-08 09:48:43 | [CL_point_env] epoch #1 | Pairwise Similarity Matrix: tensor([[ 1.0000, -0.9997],
        [-0.9997,  1.0000]], grad_fn=<MmBackward0>)
2025-01-08 09:48:43 | [CL_point_env] epoch #1 | Pairwise Similarity Matrix: tensor([[ 1.0000, -0.9997],
        [-0.9997,  1.0000]], grad_fn=<MmBackward0>)
2025-01-08 09:48:43 | [CL_point_env] epoch #1 | Training Encoder

-----------------------------  ------------
Embedding/MeanContrastiveLoss     0.0578957
PolicyTraining/MeanQ1Vals      -124.664
PolicyTraining/MeanQ2Vals      -124.664
PolicyTraining/MeanVVals       -124.557
PolicyTraining/PolicyLoss       124.627
PolicyTraining/QfLoss             0.775353
PolicyTraining/VfLoss             0.157794
-----------------------------  ------------
-----------------------------  ------------
Embedding/MeanContrastiveLoss     0.0578957
PolicyTraining/MeanQ1Vals      -124.664
PolicyTraining/MeanQ2Vals      -124.664
PolicyTraining/MeanVVals       -124.557
PolicyTraining/PolicyLoss       124.627
PolicyTraining/QfLoss             0.775353
PolicyTraining/VfLoss             0.157794
-----------------------------  ------------
2025-01-08 09:49:03 | [CL_point_env] epoch #1 | Pairwise Similarity Matrix: tensor([[ 1.0000, -0.9999],
        [-0.9999,  1.0000]], grad_fn=<MmBackward0>)
2025-01-08 09:49:03 | [CL_point_env] epoch #1 | Pairwise Similarity Matrix: tensor([[ 1.0000, -0.9999],
        [-0.9999,  1.0000]], grad_fn=<MmBackward0>)
2025-01-08 09:49:03 | [CL_point_env] epoch #1 | Training Encoder

-----------------------------  ------------
Embedding/MeanContrastiveLoss     0.0578888
PolicyTraining/MeanQ1Vals      -128.671
PolicyTraining/MeanQ2Vals      -128.671
PolicyTraining/MeanVVals       -128.608
PolicyTraining/PolicyLoss       128.696
PolicyTraining/QfLoss             0.64451
PolicyTraining/VfLoss             0.167574
-----------------------------  ------------
-----------------------------  ------------
Embedding/MeanContrastiveLoss     0.0578888
PolicyTraining/MeanQ1Vals      -128.671
PolicyTraining/MeanQ2Vals      -128.671
PolicyTraining/MeanVVals       -128.608
PolicyTraining/PolicyLoss       128.696
PolicyTraining/QfLoss             0.64451
PolicyTraining/VfLoss             0.167574
-----------------------------  ------------
2025-01-08 09:49:24 | [CL_point_env] epoch #1 | Pairwise Similarity Matrix: tensor([[ 1.0000, -0.9999],
        [-0.9999,  1.0000]], grad_fn=<MmBackward0>)
2025-01-08 09:49:24 | [CL_point_env] epoch #1 | Pairwise Similarity Matrix: tensor([[ 1.0000, -0.9999],
        [-0.9999,  1.0000]], grad_fn=<MmBackward0>)
2025-01-08 09:49:24 | [CL_point_env] epoch #1 | Training Encoder

-----------------------------  ------------
Embedding/MeanContrastiveLoss     0.0578852
PolicyTraining/MeanQ1Vals      -133.569
PolicyTraining/MeanQ2Vals      -133.569
PolicyTraining/MeanVVals       -133.545
PolicyTraining/PolicyLoss       133.639
PolicyTraining/QfLoss             0.800862
PolicyTraining/VfLoss             0.190184
-----------------------------  ------------
-----------------------------  ------------
Embedding/MeanContrastiveLoss     0.0578852
PolicyTraining/MeanQ1Vals      -133.569
PolicyTraining/MeanQ2Vals      -133.569
PolicyTraining/MeanVVals       -133.545
PolicyTraining/PolicyLoss       133.639
PolicyTraining/QfLoss             0.800862
PolicyTraining/VfLoss             0.190184
-----------------------------  ------------
2025-01-08 09:49:45 | [CL_point_env] epoch #1 | Pairwise Similarity Matrix: tensor([[ 1.0000, -0.9999],
        [-0.9999,  1.0000]], grad_fn=<MmBackward0>)
2025-01-08 09:49:45 | [CL_point_env] epoch #1 | Pairwise Similarity Matrix: tensor([[ 1.0000, -0.9999],
        [-0.9999,  1.0000]], grad_fn=<MmBackward0>)
2025-01-08 09:49:45 | [CL_point_env] epoch #1 | Training Encoder

-----------------------------  -----------
Embedding/MeanContrastiveLoss     0.057885
PolicyTraining/MeanQ1Vals      -138.393
PolicyTraining/MeanQ2Vals      -138.393
PolicyTraining/MeanVVals       -138.363
PolicyTraining/PolicyLoss       138.548
PolicyTraining/QfLoss             1.25939
PolicyTraining/VfLoss             0.22313
-----------------------------  -----------
-----------------------------  -----------
Embedding/MeanContrastiveLoss     0.057885
PolicyTraining/MeanQ1Vals      -138.393
PolicyTraining/MeanQ2Vals      -138.393
PolicyTraining/MeanVVals       -138.363
PolicyTraining/PolicyLoss       138.548
PolicyTraining/QfLoss             1.25939
PolicyTraining/VfLoss             0.22313
-----------------------------  -----------
2025-01-08 09:50:05 | [CL_point_env] epoch #1 | Pairwise Similarity Matrix: tensor([[ 1.0000, -0.9999],
        [-0.9999,  1.0000]], grad_fn=<MmBackward0>)
2025-01-08 09:50:05 | [CL_point_env] epoch #1 | Pairwise Similarity Matrix: tensor([[ 1.0000, -0.9999],
        [-0.9999,  1.0000]], grad_fn=<MmBackward0>)
2025-01-08 09:50:05 | [CL_point_env] epoch #1 | Training Encoder

-----------------------------  ------------
Embedding/MeanContrastiveLoss     0.0578855
PolicyTraining/MeanQ1Vals      -142.518
PolicyTraining/MeanQ2Vals      -142.518
PolicyTraining/MeanVVals       -142.96
PolicyTraining/PolicyLoss       142.718
PolicyTraining/QfLoss             0.884268
PolicyTraining/VfLoss             0.231165
-----------------------------  ------------
-----------------------------  ------------
Embedding/MeanContrastiveLoss     0.0578855
PolicyTraining/MeanQ1Vals      -142.518
PolicyTraining/MeanQ2Vals      -142.518
PolicyTraining/MeanVVals       -142.96
PolicyTraining/PolicyLoss       142.718
PolicyTraining/QfLoss             0.884268
PolicyTraining/VfLoss             0.231165
-----------------------------  ------------
2025-01-08 09:50:24 | [CL_point_env] epoch #1 | Evaluating...
2025-01-08 09:50:24 | [CL_point_env] epoch #1 | Sampling for adapation and meta-testing...
2025-01-08 09:50:25 | [CL_point_env] epoch #1 | Finished meta-testing...
2025-01-08 09:50:25 | [CL_point_env] epoch #1 | Sampling for adapation and meta-testing...
2025-01-08 09:50:34 | [CL_point_env] epoch #1 | Finished meta-testing...
2025-01-08 09:50:34 | [CL_point_env] epoch #1 | Saving snapshot...
2025-01-08 09:50:34 | [CL_point_env] epoch #1 | Saved
2025-01-08 09:50:34 | [CL_point_env] epoch #1 | Time 646.78 s
2025-01-08 09:50:34 | [CL_point_env] epoch #1 | EpochTime 318.22 s
--------------------------------------------------  -------------
Embedding/MeanContrastiveLoss                           0.0578855
MetaTest/Average/AverageDiscountedReturn             -357.005
MetaTest/Average/AverageReturn                       -357.005
MetaTest/Average/Iteration                              1
MetaTest/Average/MaxReturn                            -24.5149
MetaTest/Average/MinReturn                           -523.507
MetaTest/Average/NumEpisodes                           21
MetaTest/Average/StdReturn                            138.536
MetaTest/Average/SuccessRate                            0.047619
MetaTest/Average/TerminationRate                        0.047619
MetaTest/__unnamed_task__/AverageDiscountedReturn    -357.005
MetaTest/__unnamed_task__/AverageReturn              -357.005
MetaTest/__unnamed_task__/Iteration                     1
MetaTest/__unnamed_task__/MaxReturn                   -24.5149
MetaTest/__unnamed_task__/MinReturn                  -523.507
MetaTest/__unnamed_task__/NumEpisodes                  21
MetaTest/__unnamed_task__/StdReturn                   138.536
MetaTest/__unnamed_task__/SuccessRate                   0.047619
MetaTest/__unnamed_task__/TerminationRate               0.047619
MetaTrain/Average/AverageDiscountedReturn            -186.638
MetaTrain/Average/AverageReturn                      -186.638
MetaTrain/Average/Iteration                             1
MetaTrain/Average/MaxReturn                           -24.9698
MetaTrain/Average/MinReturn                          -524.063
MetaTrain/Average/NumEpisodes                           4
MetaTrain/Average/StdReturn                           203.367
MetaTrain/Average/SuccessRate                           0.5
MetaTrain/Average/TerminationRate                       0.5
MetaTrain/__unnamed_task__/AverageDiscountedReturn   -186.638
MetaTrain/__unnamed_task__/AverageReturn             -186.638
MetaTrain/__unnamed_task__/Iteration                    1
MetaTrain/__unnamed_task__/MaxReturn                  -24.9698
MetaTrain/__unnamed_task__/MinReturn                 -524.063
MetaTrain/__unnamed_task__/NumEpisodes                  4
MetaTrain/__unnamed_task__/StdReturn                  203.367
MetaTrain/__unnamed_task__/SuccessRate                  0.5
MetaTrain/__unnamed_task__/TerminationRate              0.5
PolicyTraining/MeanQ1Vals                            -142.518
PolicyTraining/MeanQ2Vals                            -142.518
PolicyTraining/MeanVVals                             -142.96
PolicyTraining/PolicyLoss                             142.718
PolicyTraining/QfLoss                                   0.884268
PolicyTraining/VfLoss                                   0.231165
TotalEnvSteps                                       11590
--------------------------------------------------  -------------
2025-01-08 09:50:35 | [CL_point_env] epoch #2 | Training...
2025-01-08 09:50:36 | [CL_point_env] epoch #2 | Pairwise Similarity Matrix: tensor([[1.0000, 1.0000],
        [1.0000, 1.0000]], grad_fn=<MmBackward0>)
2025-01-08 09:50:36 | [CL_point_env] epoch #2 | Pairwise Similarity Matrix: tensor([[1.0000, 1.0000],
        [1.0000, 1.0000]], grad_fn=<MmBackward0>)
2025-01-08 09:50:36 | [CL_point_env] epoch #2 | Training Encoder

-----------------------------  -----------
Embedding/MeanContrastiveLoss     0.153897
PolicyTraining/MeanQ1Vals      -152.479
PolicyTraining/MeanQ2Vals      -152.479
PolicyTraining/MeanVVals       -151.197
PolicyTraining/PolicyLoss       151.553
PolicyTraining/QfLoss            15.4137
PolicyTraining/VfLoss             1.52869
-----------------------------  -----------
-----------------------------  -----------
Embedding/MeanContrastiveLoss     0.153897
PolicyTraining/MeanQ1Vals      -152.479
PolicyTraining/MeanQ2Vals      -152.479
PolicyTraining/MeanVVals       -151.197
PolicyTraining/PolicyLoss       151.553
PolicyTraining/QfLoss            15.4137
PolicyTraining/VfLoss             1.52869
-----------------------------  -----------
2025-01-08 09:51:00 | [CL_point_env] epoch #2 | Pairwise Similarity Matrix: tensor([[1.0000, 1.0000],
        [1.0000, 1.0000]], grad_fn=<MmBackward0>)
2025-01-08 09:51:00 | [CL_point_env] epoch #2 | Pairwise Similarity Matrix: tensor([[1.0000, 1.0000],
        [1.0000, 1.0000]], grad_fn=<MmBackward0>)
2025-01-08 09:51:00 | [CL_point_env] epoch #2 | Training Encoder

-----------------------------  -----------
Embedding/MeanContrastiveLoss     0.153897
PolicyTraining/MeanQ1Vals      -157.884
PolicyTraining/MeanQ2Vals      -157.884
PolicyTraining/MeanVVals       -157.273
PolicyTraining/PolicyLoss       157.158
PolicyTraining/QfLoss            14.9067
PolicyTraining/VfLoss             0.480181
-----------------------------  -----------
-----------------------------  -----------
Embedding/MeanContrastiveLoss     0.153897
PolicyTraining/MeanQ1Vals      -157.884
PolicyTraining/MeanQ2Vals      -157.884
PolicyTraining/MeanVVals       -157.273
PolicyTraining/PolicyLoss       157.158
PolicyTraining/QfLoss            14.9067
PolicyTraining/VfLoss             0.480181
-----------------------------  -----------
2025-01-08 09:51:20 | [CL_point_env] epoch #2 | Pairwise Similarity Matrix: tensor([[1.0000, 1.0000],
        [1.0000, 1.0000]], grad_fn=<MmBackward0>)
2025-01-08 09:51:20 | [CL_point_env] epoch #2 | Pairwise Similarity Matrix: tensor([[1.0000, 1.0000],
        [1.0000, 1.0000]], grad_fn=<MmBackward0>)
2025-01-08 09:51:20 | [CL_point_env] epoch #2 | Training Encoder

-----------------------------  -----------
Embedding/MeanContrastiveLoss     0.153896
PolicyTraining/MeanQ1Vals      -161.942
PolicyTraining/MeanQ2Vals      -161.942
PolicyTraining/MeanVVals       -161.579
PolicyTraining/PolicyLoss       161.334
PolicyTraining/QfLoss            13.5286
PolicyTraining/VfLoss             0.522279
-----------------------------  -----------
-----------------------------  -----------
Embedding/MeanContrastiveLoss     0.153896
PolicyTraining/MeanQ1Vals      -161.942
PolicyTraining/MeanQ2Vals      -161.942
PolicyTraining/MeanVVals       -161.579
PolicyTraining/PolicyLoss       161.334
PolicyTraining/QfLoss            13.5286
PolicyTraining/VfLoss             0.522279
-----------------------------  -----------
2025-01-08 09:51:40 | [CL_point_env] epoch #2 | Pairwise Similarity Matrix: tensor([[1.0000, 1.0000],
        [1.0000, 1.0000]], grad_fn=<MmBackward0>)
2025-01-08 09:51:40 | [CL_point_env] epoch #2 | Pairwise Similarity Matrix: tensor([[1.0000, 1.0000],
        [1.0000, 1.0000]], grad_fn=<MmBackward0>)
2025-01-08 09:51:40 | [CL_point_env] epoch #2 | Training Encoder

-----------------------------  -----------
Embedding/MeanContrastiveLoss     0.153896
PolicyTraining/MeanQ1Vals      -169.016
PolicyTraining/MeanQ2Vals      -169.016
PolicyTraining/MeanVVals       -168.583
PolicyTraining/PolicyLoss       168.503
PolicyTraining/QfLoss            13.5428
PolicyTraining/VfLoss             0.404575
-----------------------------  -----------
-----------------------------  -----------
Embedding/MeanContrastiveLoss     0.153896
PolicyTraining/MeanQ1Vals      -169.016
PolicyTraining/MeanQ2Vals      -169.016
PolicyTraining/MeanVVals       -168.583
PolicyTraining/PolicyLoss       168.503
PolicyTraining/QfLoss            13.5428
PolicyTraining/VfLoss             0.404575
-----------------------------  -----------
2025-01-08 09:52:01 | [CL_point_env] epoch #2 | Pairwise Similarity Matrix: tensor([[1.0000, 1.0000],
        [1.0000, 1.0000]], grad_fn=<MmBackward0>)
2025-01-08 09:52:01 | [CL_point_env] epoch #2 | Pairwise Similarity Matrix: tensor([[1.0000, 1.0000],
        [1.0000, 1.0000]], grad_fn=<MmBackward0>)
2025-01-08 09:52:01 | [CL_point_env] epoch #2 | Training Encoder

-----------------------------  -----------
Embedding/MeanContrastiveLoss     0.153896
PolicyTraining/MeanQ1Vals      -170.987
PolicyTraining/MeanQ2Vals      -170.987
PolicyTraining/MeanVVals       -170.384
PolicyTraining/PolicyLoss       170.609
PolicyTraining/QfLoss            12.481
PolicyTraining/VfLoss             0.434759
-----------------------------  -----------
-----------------------------  -----------
Embedding/MeanContrastiveLoss     0.153896
PolicyTraining/MeanQ1Vals      -170.987
PolicyTraining/MeanQ2Vals      -170.987
PolicyTraining/MeanVVals       -170.384
PolicyTraining/PolicyLoss       170.609
PolicyTraining/QfLoss            12.481
PolicyTraining/VfLoss             0.434759
-----------------------------  -----------
2025-01-08 09:52:21 | [CL_point_env] epoch #2 | Pairwise Similarity Matrix: tensor([[1.0000, 1.0000],
        [1.0000, 1.0000]], grad_fn=<MmBackward0>)
2025-01-08 09:52:21 | [CL_point_env] epoch #2 | Pairwise Similarity Matrix: tensor([[1.0000, 1.0000],
        [1.0000, 1.0000]], grad_fn=<MmBackward0>)
2025-01-08 09:52:21 | [CL_point_env] epoch #2 | Training Encoder

-----------------------------  -----------
Embedding/MeanContrastiveLoss     0.153896
PolicyTraining/MeanQ1Vals      -180.823
PolicyTraining/MeanQ2Vals      -180.823
PolicyTraining/MeanVVals       -180.488
PolicyTraining/PolicyLoss       180.492
PolicyTraining/QfLoss            12.6139
PolicyTraining/VfLoss             0.453435
-----------------------------  -----------
-----------------------------  -----------
Embedding/MeanContrastiveLoss     0.153896
PolicyTraining/MeanQ1Vals      -180.823
PolicyTraining/MeanQ2Vals      -180.823
PolicyTraining/MeanVVals       -180.488
PolicyTraining/PolicyLoss       180.492
PolicyTraining/QfLoss            12.6139
PolicyTraining/VfLoss             0.453435
-----------------------------  -----------
2025-01-08 09:52:41 | [CL_point_env] epoch #2 | Pairwise Similarity Matrix: tensor([[1.0000, 1.0000],
        [1.0000, 1.0000]], grad_fn=<MmBackward0>)
2025-01-08 09:52:41 | [CL_point_env] epoch #2 | Pairwise Similarity Matrix: tensor([[1.0000, 1.0000],
        [1.0000, 1.0000]], grad_fn=<MmBackward0>)
2025-01-08 09:52:41 | [CL_point_env] epoch #2 | Training Encoder

-----------------------------  -----------
Embedding/MeanContrastiveLoss     0.153896
PolicyTraining/MeanQ1Vals      -182.648
PolicyTraining/MeanQ2Vals      -182.648
PolicyTraining/MeanVVals       -182.183
PolicyTraining/PolicyLoss       182.403
PolicyTraining/QfLoss            12.4406
PolicyTraining/VfLoss             0.47523
-----------------------------  -----------
-----------------------------  -----------
Embedding/MeanContrastiveLoss     0.153896
PolicyTraining/MeanQ1Vals      -182.648
PolicyTraining/MeanQ2Vals      -182.648
PolicyTraining/MeanVVals       -182.183
PolicyTraining/PolicyLoss       182.403
PolicyTraining/QfLoss            12.4406
PolicyTraining/VfLoss             0.47523
-----------------------------  -----------
2025-01-08 09:53:01 | [CL_point_env] epoch #2 | Pairwise Similarity Matrix: tensor([[1.0000, 1.0000],
        [1.0000, 1.0000]], grad_fn=<MmBackward0>)
2025-01-08 09:53:01 | [CL_point_env] epoch #2 | Pairwise Similarity Matrix: tensor([[1.0000, 1.0000],
        [1.0000, 1.0000]], grad_fn=<MmBackward0>)
2025-01-08 09:53:01 | [CL_point_env] epoch #2 | Training Encoder

-----------------------------  -----------
Embedding/MeanContrastiveLoss     0.153896
PolicyTraining/MeanQ1Vals      -184.055
PolicyTraining/MeanQ2Vals      -184.055
PolicyTraining/MeanVVals       -183.719
PolicyTraining/PolicyLoss       183.865
PolicyTraining/QfLoss            13.407
PolicyTraining/VfLoss             0.396319
-----------------------------  -----------
-----------------------------  -----------
Embedding/MeanContrastiveLoss     0.153896
PolicyTraining/MeanQ1Vals      -184.055
PolicyTraining/MeanQ2Vals      -184.055
PolicyTraining/MeanVVals       -183.719
PolicyTraining/PolicyLoss       183.865
PolicyTraining/QfLoss            13.407
PolicyTraining/VfLoss             0.396319
-----------------------------  -----------
2025-01-08 09:53:21 | [CL_point_env] epoch #2 | Pairwise Similarity Matrix: tensor([[1.0000, 1.0000],
        [1.0000, 1.0000]], grad_fn=<MmBackward0>)
2025-01-08 09:53:21 | [CL_point_env] epoch #2 | Pairwise Similarity Matrix: tensor([[1.0000, 1.0000],
        [1.0000, 1.0000]], grad_fn=<MmBackward0>)
2025-01-08 09:53:21 | [CL_point_env] epoch #2 | Training Encoder

-----------------------------  -----------
Embedding/MeanContrastiveLoss     0.153895
PolicyTraining/MeanQ1Vals      -188.846
PolicyTraining/MeanQ2Vals      -188.846
PolicyTraining/MeanVVals       -188.687
PolicyTraining/PolicyLoss       188.694
PolicyTraining/QfLoss            11.6448
PolicyTraining/VfLoss             0.401648
-----------------------------  -----------
-----------------------------  -----------
Embedding/MeanContrastiveLoss     0.153895
PolicyTraining/MeanQ1Vals      -188.846
PolicyTraining/MeanQ2Vals      -188.846
PolicyTraining/MeanVVals       -188.687
PolicyTraining/PolicyLoss       188.694
PolicyTraining/QfLoss            11.6448
PolicyTraining/VfLoss             0.401648
-----------------------------  -----------
2025-01-08 09:53:41 | [CL_point_env] epoch #2 | Pairwise Similarity Matrix: tensor([[1.0000, 1.0000],
        [1.0000, 1.0000]], grad_fn=<MmBackward0>)
2025-01-08 09:53:41 | [CL_point_env] epoch #2 | Pairwise Similarity Matrix: tensor([[1.0000, 1.0000],
        [1.0000, 1.0000]], grad_fn=<MmBackward0>)
2025-01-08 09:53:41 | [CL_point_env] epoch #2 | Training Encoder

-----------------------------  -----------
Embedding/MeanContrastiveLoss     0.153895
PolicyTraining/MeanQ1Vals      -186.793
PolicyTraining/MeanQ2Vals      -186.793
PolicyTraining/MeanVVals       -186.738
PolicyTraining/PolicyLoss       186.917
PolicyTraining/QfLoss             9.71308
PolicyTraining/VfLoss             0.61495
-----------------------------  -----------
-----------------------------  -----------
Embedding/MeanContrastiveLoss     0.153895
PolicyTraining/MeanQ1Vals      -186.793
PolicyTraining/MeanQ2Vals      -186.793
PolicyTraining/MeanVVals       -186.738
PolicyTraining/PolicyLoss       186.917
PolicyTraining/QfLoss             9.71308
PolicyTraining/VfLoss             0.61495
-----------------------------  -----------
2025-01-08 09:54:01 | [CL_point_env] epoch #2 | Pairwise Similarity Matrix: tensor([[1.0000, 1.0000],
        [1.0000, 1.0000]], grad_fn=<MmBackward0>)
2025-01-08 09:54:01 | [CL_point_env] epoch #2 | Pairwise Similarity Matrix: tensor([[1.0000, 1.0000],
        [1.0000, 1.0000]], grad_fn=<MmBackward0>)
2025-01-08 09:54:01 | [CL_point_env] epoch #2 | Training Encoder

-----------------------------  -----------
Embedding/MeanContrastiveLoss     0.153894
PolicyTraining/MeanQ1Vals      -200.046
PolicyTraining/MeanQ2Vals      -200.046
PolicyTraining/MeanVVals       -200.058
PolicyTraining/PolicyLoss       200.008
PolicyTraining/QfLoss            13.1706
PolicyTraining/VfLoss             0.57396
-----------------------------  -----------
-----------------------------  -----------
Embedding/MeanContrastiveLoss     0.153894
PolicyTraining/MeanQ1Vals      -200.046
PolicyTraining/MeanQ2Vals      -200.046
PolicyTraining/MeanVVals       -200.058
PolicyTraining/PolicyLoss       200.008
PolicyTraining/QfLoss            13.1706
PolicyTraining/VfLoss             0.57396
-----------------------------  -----------
2025-01-08 09:54:21 | [CL_point_env] epoch #2 | Pairwise Similarity Matrix: tensor([[1.0000, 0.9999],
        [0.9999, 1.0000]], grad_fn=<MmBackward0>)
2025-01-08 09:54:21 | [CL_point_env] epoch #2 | Pairwise Similarity Matrix: tensor([[1.0000, 0.9999],
        [0.9999, 1.0000]], grad_fn=<MmBackward0>)
2025-01-08 09:54:21 | [CL_point_env] epoch #2 | Training Encoder

-----------------------------  -----------
Embedding/MeanContrastiveLoss     0.153893
PolicyTraining/MeanQ1Vals      -200.468
PolicyTraining/MeanQ2Vals      -200.468
PolicyTraining/MeanVVals       -199.966
PolicyTraining/PolicyLoss       200.521
PolicyTraining/QfLoss            12.2614
PolicyTraining/VfLoss             1.02243
-----------------------------  -----------
-----------------------------  -----------
Embedding/MeanContrastiveLoss     0.153893
PolicyTraining/MeanQ1Vals      -200.468
PolicyTraining/MeanQ2Vals      -200.468
PolicyTraining/MeanVVals       -199.966
PolicyTraining/PolicyLoss       200.521
PolicyTraining/QfLoss            12.2614
PolicyTraining/VfLoss             1.02243
-----------------------------  -----------
2025-01-08 09:54:42 | [CL_point_env] epoch #2 | Pairwise Similarity Matrix: tensor([[1.0000, 0.9999],
        [0.9999, 1.0000]], grad_fn=<MmBackward0>)
2025-01-08 09:54:42 | [CL_point_env] epoch #2 | Pairwise Similarity Matrix: tensor([[1.0000, 0.9999],
        [0.9999, 1.0000]], grad_fn=<MmBackward0>)
2025-01-08 09:54:42 | [CL_point_env] epoch #2 | Training Encoder

-----------------------------  -----------
Embedding/MeanContrastiveLoss     0.153893
PolicyTraining/MeanQ1Vals      -213.417
PolicyTraining/MeanQ2Vals      -213.417
PolicyTraining/MeanVVals       -213.74
PolicyTraining/PolicyLoss       213.561
PolicyTraining/QfLoss            11.2373
PolicyTraining/VfLoss             0.961469
-----------------------------  -----------
-----------------------------  -----------
Embedding/MeanContrastiveLoss     0.153893
PolicyTraining/MeanQ1Vals      -213.417
PolicyTraining/MeanQ2Vals      -213.417
PolicyTraining/MeanVVals       -213.74
PolicyTraining/PolicyLoss       213.561
PolicyTraining/QfLoss            11.2373
PolicyTraining/VfLoss             0.961469
-----------------------------  -----------
2025-01-08 09:55:02 | [CL_point_env] epoch #2 | Pairwise Similarity Matrix: tensor([[1.0000, 0.9999],
        [0.9999, 1.0000]], grad_fn=<MmBackward0>)
2025-01-08 09:55:02 | [CL_point_env] epoch #2 | Pairwise Similarity Matrix: tensor([[1.0000, 0.9999],
        [0.9999, 1.0000]], grad_fn=<MmBackward0>)
2025-01-08 09:55:02 | [CL_point_env] epoch #2 | Training Encoder

-----------------------------  -----------
Embedding/MeanContrastiveLoss     0.153889
PolicyTraining/MeanQ1Vals      -208.963
PolicyTraining/MeanQ2Vals      -208.963
PolicyTraining/MeanVVals       -209.855
PolicyTraining/PolicyLoss       209.096
PolicyTraining/QfLoss            12.8423
PolicyTraining/VfLoss             1.69125
-----------------------------  -----------
-----------------------------  -----------
Embedding/MeanContrastiveLoss     0.153889
PolicyTraining/MeanQ1Vals      -208.963
PolicyTraining/MeanQ2Vals      -208.963
PolicyTraining/MeanVVals       -209.855
PolicyTraining/PolicyLoss       209.096
PolicyTraining/QfLoss            12.8423
PolicyTraining/VfLoss             1.69125
-----------------------------  -----------
2025-01-08 09:55:23 | [CL_point_env] epoch #2 | Pairwise Similarity Matrix: tensor([[1.0000, 0.9994],
        [0.9994, 1.0000]], grad_fn=<MmBackward0>)
2025-01-08 09:55:23 | [CL_point_env] epoch #2 | Pairwise Similarity Matrix: tensor([[1.0000, 0.9994],
        [0.9994, 1.0000]], grad_fn=<MmBackward0>)
2025-01-08 09:55:23 | [CL_point_env] epoch #2 | Training Encoder

-----------------------------  -----------
Embedding/MeanContrastiveLoss     0.153882
PolicyTraining/MeanQ1Vals      -218.613
PolicyTraining/MeanQ2Vals      -218.613
PolicyTraining/MeanVVals       -218.094
PolicyTraining/PolicyLoss       218.769
PolicyTraining/QfLoss            13.6202
PolicyTraining/VfLoss             1.63233
-----------------------------  -----------
-----------------------------  -----------
Embedding/MeanContrastiveLoss     0.153882
PolicyTraining/MeanQ1Vals      -218.613
PolicyTraining/MeanQ2Vals      -218.613
PolicyTraining/MeanVVals       -218.094
PolicyTraining/PolicyLoss       218.769
PolicyTraining/QfLoss            13.6202
PolicyTraining/VfLoss             1.63233
-----------------------------  -----------
2025-01-08 09:55:42 | [CL_point_env] epoch #2 | Evaluating...
2025-01-08 09:55:42 | [CL_point_env] epoch #2 | Sampling for adapation and meta-testing...
2025-01-08 09:55:43 | [CL_point_env] epoch #2 | Finished meta-testing...
2025-01-08 09:55:43 | [CL_point_env] epoch #2 | Sampling for adapation and meta-testing...
2025-01-08 09:55:52 | [CL_point_env] epoch #2 | Finished meta-testing...
2025-01-08 09:55:52 | [CL_point_env] epoch #2 | Saving snapshot...
2025-01-08 09:55:52 | [CL_point_env] epoch #2 | Saved
2025-01-08 09:55:52 | [CL_point_env] epoch #2 | Time 965.20 s
2025-01-08 09:55:52 | [CL_point_env] epoch #2 | EpochTime 318.33 s
--------------------------------------------------  ------------
Embedding/MeanContrastiveLoss                           0.153882
MetaTest/Average/AverageDiscountedReturn             -143.589
MetaTest/Average/AverageReturn                       -143.589
MetaTest/Average/Iteration                              2
MetaTest/Average/MaxReturn                            -24.5682
MetaTest/Average/MinReturn                           -363.599
MetaTest/Average/NumEpisodes                           24
MetaTest/Average/StdReturn                             93.3003
MetaTest/Average/SuccessRate                            0.208333
MetaTest/Average/TerminationRate                        0.208333
MetaTest/__unnamed_task__/AverageDiscountedReturn    -143.589
MetaTest/__unnamed_task__/AverageReturn              -143.589
MetaTest/__unnamed_task__/Iteration                     2
MetaTest/__unnamed_task__/MaxReturn                   -24.5682
MetaTest/__unnamed_task__/MinReturn                  -363.599
MetaTest/__unnamed_task__/NumEpisodes                  24
MetaTest/__unnamed_task__/StdReturn                    93.3003
MetaTest/__unnamed_task__/SuccessRate                   0.208333
MetaTest/__unnamed_task__/TerminationRate               0.208333
MetaTrain/Average/AverageDiscountedReturn             -36.3481
MetaTrain/Average/AverageReturn                       -36.3481
MetaTrain/Average/Iteration                             2
MetaTrain/Average/MaxReturn                           -25.3018
MetaTrain/Average/MinReturn                           -53.4751
MetaTrain/Average/NumEpisodes                           5
MetaTrain/Average/StdReturn                             9.84945
MetaTrain/Average/SuccessRate                           0.8
MetaTrain/Average/TerminationRate                       0.8
MetaTrain/__unnamed_task__/AverageDiscountedReturn    -36.3481
MetaTrain/__unnamed_task__/AverageReturn              -36.3481
MetaTrain/__unnamed_task__/Iteration                    2
MetaTrain/__unnamed_task__/MaxReturn                  -25.3018
MetaTrain/__unnamed_task__/MinReturn                  -53.4751
MetaTrain/__unnamed_task__/NumEpisodes                  5
MetaTrain/__unnamed_task__/StdReturn                    9.84945
MetaTrain/__unnamed_task__/SuccessRate                  0.8
MetaTrain/__unnamed_task__/TerminationRate              0.8
PolicyTraining/MeanQ1Vals                            -218.613
PolicyTraining/MeanQ2Vals                            -218.613
PolicyTraining/MeanVVals                             -218.094
PolicyTraining/PolicyLoss                             218.769
PolicyTraining/QfLoss                                  13.6202
PolicyTraining/VfLoss                                   1.63233
TotalEnvSteps                                       16168
--------------------------------------------------  ------------

2025-01-08 09:26:10 | [CL_point_env] Logging to /Users/paulnitschke/Desktop/projects/geo_meta_rl/data/local/experiment/CL_point_env_27


Failed to import TF-Keras. Please note that TF-Keras is not installed by default when you install TensorFlow Probability. This is so that JAX-only users do not have to install TensorFlow or TF-Keras. To use TensorFlow Probability with TensorFlow, please install the tf-keras or tf-keras-nightly package.
This can be be done through installing the tensorflow-probability[tf] extra.
/Users/paulnitschke/Desktop/projects/geo_meta_rl/garage/experiment/deterministic.py:36: UserWarning: Enabeling deterministic mode in PyTorch can have a performance impact when using GPU.
  warnings.warn(


2025-01-08 09:26:11 | [CL_point_env] Obtaining samples...
/Users/paulnitschke/miniconda3/envs/env_geo_meta_rl/lib/python3.9/site-packages/torch/distributions/distribution.py:53: UserWarning: <class 'garage.torch.distributions.tanh_normal.TanhNormal'> does not define `arg_constraints`. Please set `arg_constraints = {}` or initialize the distribution with `validate_args=False` to turn off validation.
  warnings.warn(
/Users/paulnitschke/Desktop/projects/geo_meta_rl/garage/_dtypes.py:1051: UserWarning: Observation array([0., 0., 2.]) is outside observation_space Box(-inf, inf, (3,), float32)
  warnings.warn(
2025-01-08 09:26:13 | [CL_point_env] epoch #0 | Pre-Training Encoder...
2025-01-08 09:26:14 | [CL_point_env] epoch #0 | Pairwise Similarity Matrix: tensor([[1.0000, 0.9984],
        [0.9984, 1.0000]], grad_fn=<MmBackward0>)
2025-01-08 09:26:25 | [CL_point_env] epoch #0 | Pairwise Similarity Matrix: tensor([[ 1.0000, -0.9989],
        [-0.9989,  1.0000]], grad_fn=<MmBackward0>)
2025-01-08 09:26:25 | [CL_point_env] epoch #0 | Pre-Training Encoder Done...
2025-01-08 09:26:26 | [CL_point_env] epoch #0 | Pairwise Similarity Matrix: tensor([[ 1.0000, -0.9992],
        [-0.9992,  1.0000]], grad_fn=<MmBackward0>)
2025-01-08 09:26:26 | [CL_point_env] epoch #0 | Training...
2025-01-08 09:26:26 | [CL_point_env] epoch #0 | Pairwise Similarity Matrix: tensor([[ 1.0000, -0.9995],
        [-0.9995,  1.0000]], grad_fn=<MmBackward0>)
2025-01-08 09:26:27 | [CL_point_env] epoch #0 | Training Encoder

-----------------------------  -----------
Embedding/MeanContrastiveLoss    0.0579126
PolicyTraining/MeanQ1Vals      -22.432
PolicyTraining/MeanQ2Vals      -22.432
PolicyTraining/MeanVVals        -4.44753
PolicyTraining/PolicyLoss       21.2361
PolicyTraining/QfLoss           13.0887
PolicyTraining/VfLoss          294.253
-----------------------------  -----------
-----------------------------  -----------
Embedding/MeanContrastiveLoss    0.0579126
PolicyTraining/MeanQ1Vals      -22.432
PolicyTraining/MeanQ2Vals      -22.432
PolicyTraining/MeanVVals        -4.44753
PolicyTraining/PolicyLoss       21.2361
PolicyTraining/QfLoss           13.0887
PolicyTraining/VfLoss          294.253
-----------------------------  -----------
2025-01-08 09:26:47 | [CL_point_env] epoch #0 | Pairwise Similarity Matrix: tensor([[ 1.0000, -0.9996],
        [-0.9996,  1.0000]], grad_fn=<MmBackward0>)
2025-01-08 09:26:47 | [CL_point_env] epoch #0 | Training Encoder

-----------------------------  -----------
Embedding/MeanContrastiveLoss    0.0578983
PolicyTraining/MeanQ1Vals      -22.4682
PolicyTraining/MeanQ2Vals      -22.4682
PolicyTraining/MeanVVals       -20.0202
PolicyTraining/PolicyLoss       21.225
PolicyTraining/QfLoss            5.27026
PolicyTraining/VfLoss            2.12084
-----------------------------  -----------
-----------------------------  -----------
Embedding/MeanContrastiveLoss    0.0578983
PolicyTraining/MeanQ1Vals      -22.4682
PolicyTraining/MeanQ2Vals      -22.4682
PolicyTraining/MeanVVals       -20.0202
PolicyTraining/PolicyLoss       21.225
PolicyTraining/QfLoss            5.27026
PolicyTraining/VfLoss            2.12084
-----------------------------  -----------
2025-01-08 09:27:07 | [CL_point_env] epoch #0 | Pairwise Similarity Matrix: tensor([[ 1.0000, -0.9997],
        [-0.9997,  1.0000]], grad_fn=<MmBackward0>)
2025-01-08 09:27:07 | [CL_point_env] epoch #0 | Training Encoder

-----------------------------  -----------
Embedding/MeanContrastiveLoss    0.0579034
PolicyTraining/MeanQ1Vals      -24.3559
PolicyTraining/MeanQ2Vals      -24.3559
PolicyTraining/MeanVVals       -23.0708
PolicyTraining/PolicyLoss       23.1091
PolicyTraining/QfLoss            1.45557
PolicyTraining/VfLoss            0.999558
-----------------------------  -----------
-----------------------------  -----------
Embedding/MeanContrastiveLoss    0.0579034
PolicyTraining/MeanQ1Vals      -24.3559
PolicyTraining/MeanQ2Vals      -24.3559
PolicyTraining/MeanVVals       -23.0708
PolicyTraining/PolicyLoss       23.1091
PolicyTraining/QfLoss            1.45557
PolicyTraining/VfLoss            0.999558
-----------------------------  -----------
2025-01-08 09:27:27 | [CL_point_env] epoch #0 | Pairwise Similarity Matrix: tensor([[ 1.0000, -0.9999],
        [-0.9999,  1.0000]], grad_fn=<MmBackward0>)
2025-01-08 09:27:27 | [CL_point_env] epoch #0 | Training Encoder

-----------------------------  -----------
Embedding/MeanContrastiveLoss    0.0578935
PolicyTraining/MeanQ1Vals      -26.7977
PolicyTraining/MeanQ2Vals      -26.7977
PolicyTraining/MeanVVals       -25.6567
PolicyTraining/PolicyLoss       25.6114
PolicyTraining/QfLoss            0.485083
PolicyTraining/VfLoss            0.89481
-----------------------------  -----------
-----------------------------  -----------
Embedding/MeanContrastiveLoss    0.0578935
PolicyTraining/MeanQ1Vals      -26.7977
PolicyTraining/MeanQ2Vals      -26.7977
PolicyTraining/MeanVVals       -25.6567
PolicyTraining/PolicyLoss       25.6114
PolicyTraining/QfLoss            0.485083
PolicyTraining/VfLoss            0.89481
-----------------------------  -----------
2025-01-08 09:27:47 | [CL_point_env] epoch #0 | Pairwise Similarity Matrix: tensor([[ 1.0000, -0.9998],
        [-0.9998,  1.0000]], grad_fn=<MmBackward0>)
2025-01-08 09:27:47 | [CL_point_env] epoch #0 | Training Encoder

-----------------------------  -----------
Embedding/MeanContrastiveLoss    0.0578926
PolicyTraining/MeanQ1Vals      -30.7752
PolicyTraining/MeanQ2Vals      -30.7752
PolicyTraining/MeanVVals       -29.6161
PolicyTraining/PolicyLoss       29.6354
PolicyTraining/QfLoss            0.295163
PolicyTraining/VfLoss            0.113008
-----------------------------  -----------
-----------------------------  -----------
Embedding/MeanContrastiveLoss    0.0578926
PolicyTraining/MeanQ1Vals      -30.7752
PolicyTraining/MeanQ2Vals      -30.7752
PolicyTraining/MeanVVals       -29.6161
PolicyTraining/PolicyLoss       29.6354
PolicyTraining/QfLoss            0.295163
PolicyTraining/VfLoss            0.113008
-----------------------------  -----------
2025-01-08 09:28:07 | [CL_point_env] epoch #0 | Pairwise Similarity Matrix: tensor([[ 1.0000, -0.9999],
        [-0.9999,  1.0000]], grad_fn=<MmBackward0>)
2025-01-08 09:28:07 | [CL_point_env] epoch #0 | Training Encoder

-----------------------------  -----------
Embedding/MeanContrastiveLoss    0.0578928
PolicyTraining/MeanQ1Vals      -33.8309
PolicyTraining/MeanQ2Vals      -33.8309
PolicyTraining/MeanVVals       -32.7075
PolicyTraining/PolicyLoss       32.7265
PolicyTraining/QfLoss            0.208861
PolicyTraining/VfLoss            0.0758786
-----------------------------  -----------
-----------------------------  -----------
Embedding/MeanContrastiveLoss    0.0578928
PolicyTraining/MeanQ1Vals      -33.8309
PolicyTraining/MeanQ2Vals      -33.8309
PolicyTraining/MeanVVals       -32.7075
PolicyTraining/PolicyLoss       32.7265
PolicyTraining/QfLoss            0.208861
PolicyTraining/VfLoss            0.0758786
-----------------------------  -----------
2025-01-08 09:28:27 | [CL_point_env] epoch #0 | Pairwise Similarity Matrix: tensor([[ 1.0000, -1.0000],
        [-1.0000,  1.0000]], grad_fn=<MmBackward0>)
2025-01-08 09:28:27 | [CL_point_env] epoch #0 | Training Encoder

-----------------------------  -----------
Embedding/MeanContrastiveLoss    0.0578884
PolicyTraining/MeanQ1Vals      -34.8297
PolicyTraining/MeanQ2Vals      -34.8297
PolicyTraining/MeanVVals       -33.7868
PolicyTraining/PolicyLoss       33.7824
PolicyTraining/QfLoss            0.136801
PolicyTraining/VfLoss            0.0723519
-----------------------------  -----------
-----------------------------  -----------
Embedding/MeanContrastiveLoss    0.0578884
PolicyTraining/MeanQ1Vals      -34.8297
PolicyTraining/MeanQ2Vals      -34.8297
PolicyTraining/MeanVVals       -33.7868
PolicyTraining/PolicyLoss       33.7824
PolicyTraining/QfLoss            0.136801
PolicyTraining/VfLoss            0.0723519
-----------------------------  -----------
2025-01-08 09:28:47 | [CL_point_env] epoch #0 | Pairwise Similarity Matrix: tensor([[ 1.0000, -0.9999],
        [-0.9999,  1.0000]], grad_fn=<MmBackward0>)
2025-01-08 09:28:47 | [CL_point_env] epoch #0 | Training Encoder

-----------------------------  -----------
Embedding/MeanContrastiveLoss    0.0578893
PolicyTraining/MeanQ1Vals      -39.3191
PolicyTraining/MeanQ2Vals      -39.3191
PolicyTraining/MeanVVals       -38.3398
PolicyTraining/PolicyLoss       38.3496
PolicyTraining/QfLoss            0.105255
PolicyTraining/VfLoss            0.0640955
-----------------------------  -----------
-----------------------------  -----------
Embedding/MeanContrastiveLoss    0.0578893
PolicyTraining/MeanQ1Vals      -39.3191
PolicyTraining/MeanQ2Vals      -39.3191
PolicyTraining/MeanVVals       -38.3398
PolicyTraining/PolicyLoss       38.3496
PolicyTraining/QfLoss            0.105255
PolicyTraining/VfLoss            0.0640955
-----------------------------  -----------
2025-01-08 09:29:07 | [CL_point_env] epoch #0 | Pairwise Similarity Matrix: tensor([[ 1.0000, -1.0000],
        [-1.0000,  1.0000]], grad_fn=<MmBackward0>)
2025-01-08 09:29:07 | [CL_point_env] epoch #0 | Training Encoder

-----------------------------  -----------
Embedding/MeanContrastiveLoss    0.0578889
PolicyTraining/MeanQ1Vals      -41.9295
PolicyTraining/MeanQ2Vals      -41.9295
PolicyTraining/MeanVVals       -41.0283
PolicyTraining/PolicyLoss       41.0142
PolicyTraining/QfLoss            0.10244
PolicyTraining/VfLoss            0.0648677
-----------------------------  -----------
-----------------------------  -----------
Embedding/MeanContrastiveLoss    0.0578889
PolicyTraining/MeanQ1Vals      -41.9295
PolicyTraining/MeanQ2Vals      -41.9295
PolicyTraining/MeanVVals       -41.0283
PolicyTraining/PolicyLoss       41.0142
PolicyTraining/QfLoss            0.10244
PolicyTraining/VfLoss            0.0648677
-----------------------------  -----------
2025-01-08 09:29:28 | [CL_point_env] epoch #0 | Pairwise Similarity Matrix: tensor([[ 1.0000, -1.0000],
        [-1.0000,  1.0000]], grad_fn=<MmBackward0>)
2025-01-08 09:29:28 | [CL_point_env] epoch #0 | Training Encoder

-----------------------------  -----------
Embedding/MeanContrastiveLoss    0.0578864
PolicyTraining/MeanQ1Vals      -45.1105
PolicyTraining/MeanQ2Vals      -45.1105
PolicyTraining/MeanVVals       -44.2157
PolicyTraining/PolicyLoss       44.2664
PolicyTraining/QfLoss            0.0868079
PolicyTraining/VfLoss            0.0710359
-----------------------------  -----------
-----------------------------  -----------
Embedding/MeanContrastiveLoss    0.0578864
PolicyTraining/MeanQ1Vals      -45.1105
PolicyTraining/MeanQ2Vals      -45.1105
PolicyTraining/MeanVVals       -44.2157
PolicyTraining/PolicyLoss       44.2664
PolicyTraining/QfLoss            0.0868079
PolicyTraining/VfLoss            0.0710359
-----------------------------  -----------
2025-01-08 09:29:48 | [CL_point_env] epoch #0 | Pairwise Similarity Matrix: tensor([[ 1.0000, -1.0000],
        [-1.0000,  1.0000]], grad_fn=<MmBackward0>)
2025-01-08 09:29:48 | [CL_point_env] epoch #0 | Training Encoder

-----------------------------  -----------
Embedding/MeanContrastiveLoss    0.0578875
PolicyTraining/MeanQ1Vals      -51.35
PolicyTraining/MeanQ2Vals      -51.35
PolicyTraining/MeanVVals       -50.6032
PolicyTraining/PolicyLoss       50.6139
PolicyTraining/QfLoss            0.0782334
PolicyTraining/VfLoss            0.0590598
-----------------------------  -----------
-----------------------------  -----------
Embedding/MeanContrastiveLoss    0.0578875
PolicyTraining/MeanQ1Vals      -51.35
PolicyTraining/MeanQ2Vals      -51.35
PolicyTraining/MeanVVals       -50.6032
PolicyTraining/PolicyLoss       50.6139
PolicyTraining/QfLoss            0.0782334
PolicyTraining/VfLoss            0.0590598
-----------------------------  -----------
2025-01-08 09:30:08 | [CL_point_env] epoch #0 | Pairwise Similarity Matrix: tensor([[ 1.0000, -1.0000],
        [-1.0000,  1.0000]], grad_fn=<MmBackward0>)
2025-01-08 09:30:08 | [CL_point_env] epoch #0 | Training Encoder

-----------------------------  -----------
Embedding/MeanContrastiveLoss    0.0578864
PolicyTraining/MeanQ1Vals      -55.1313
PolicyTraining/MeanQ2Vals      -55.1313
PolicyTraining/MeanVVals       -54.4172
PolicyTraining/PolicyLoss       54.4564
PolicyTraining/QfLoss            0.0725208
PolicyTraining/VfLoss            0.069953
-----------------------------  -----------
-----------------------------  -----------
Embedding/MeanContrastiveLoss    0.0578864
PolicyTraining/MeanQ1Vals      -55.1313
PolicyTraining/MeanQ2Vals      -55.1313
PolicyTraining/MeanVVals       -54.4172
PolicyTraining/PolicyLoss       54.4564
PolicyTraining/QfLoss            0.0725208
PolicyTraining/VfLoss            0.069953
-----------------------------  -----------
2025-01-08 09:30:28 | [CL_point_env] epoch #0 | Pairwise Similarity Matrix: tensor([[ 1.0000, -1.0000],
        [-1.0000,  1.0000]], grad_fn=<MmBackward0>)
2025-01-08 09:30:28 | [CL_point_env] epoch #0 | Training Encoder

-----------------------------  -----------
Embedding/MeanContrastiveLoss    0.0578858
PolicyTraining/MeanQ1Vals      -56.8077
PolicyTraining/MeanQ2Vals      -56.8077
PolicyTraining/MeanVVals       -56.3087
PolicyTraining/PolicyLoss       56.2309
PolicyTraining/QfLoss            0.0737983
PolicyTraining/VfLoss            0.0646639
-----------------------------  -----------
-----------------------------  -----------
Embedding/MeanContrastiveLoss    0.0578858
PolicyTraining/MeanQ1Vals      -56.8077
PolicyTraining/MeanQ2Vals      -56.8077
PolicyTraining/MeanVVals       -56.3087
PolicyTraining/PolicyLoss       56.2309
PolicyTraining/QfLoss            0.0737983
PolicyTraining/VfLoss            0.0646639
-----------------------------  -----------
2025-01-08 09:30:48 | [CL_point_env] epoch #0 | Pairwise Similarity Matrix: tensor([[ 1.0000, -1.0000],
        [-1.0000,  1.0000]], grad_fn=<MmBackward0>)
2025-01-08 09:30:48 | [CL_point_env] epoch #0 | Training Encoder

-----------------------------  -----------
Embedding/MeanContrastiveLoss    0.0578864
PolicyTraining/MeanQ1Vals      -59.0754
PolicyTraining/MeanQ2Vals      -59.0754
PolicyTraining/MeanVVals       -58.5411
PolicyTraining/PolicyLoss       58.5598
PolicyTraining/QfLoss            0.0779131
PolicyTraining/VfLoss            0.0692206
-----------------------------  -----------
-----------------------------  -----------
Embedding/MeanContrastiveLoss    0.0578864
PolicyTraining/MeanQ1Vals      -59.0754
PolicyTraining/MeanQ2Vals      -59.0754
PolicyTraining/MeanVVals       -58.5411
PolicyTraining/PolicyLoss       58.5598
PolicyTraining/QfLoss            0.0779131
PolicyTraining/VfLoss            0.0692206
-----------------------------  -----------
2025-01-08 09:31:09 | [CL_point_env] epoch #0 | Pairwise Similarity Matrix: tensor([[ 1.0000, -1.0000],
        [-1.0000,  1.0000]], grad_fn=<MmBackward0>)
2025-01-08 09:31:09 | [CL_point_env] epoch #0 | Training Encoder

-----------------------------  -----------
Embedding/MeanContrastiveLoss    0.0578852
PolicyTraining/MeanQ1Vals      -62.6757
PolicyTraining/MeanQ2Vals      -62.6757
PolicyTraining/MeanVVals       -62.2326
PolicyTraining/PolicyLoss       62.2317
PolicyTraining/QfLoss            0.0677262
PolicyTraining/VfLoss            0.0717044
-----------------------------  -----------
-----------------------------  -----------
Embedding/MeanContrastiveLoss    0.0578852
PolicyTraining/MeanQ1Vals      -62.6757
PolicyTraining/MeanQ2Vals      -62.6757
PolicyTraining/MeanVVals       -62.2326
PolicyTraining/PolicyLoss       62.2317
PolicyTraining/QfLoss            0.0677262
PolicyTraining/VfLoss            0.0717044
-----------------------------  -----------
2025-01-08 09:31:28 | [CL_point_env] epoch #0 | Evaluating...
2025-01-08 09:31:28 | [CL_point_env] epoch #0 | Sampling for adapation and meta-testing...


Failed to import TF-Keras. Please note that TF-Keras is not installed by default when you install TensorFlow Probability. This is so that JAX-only users do not have to install TensorFlow or TF-Keras. To use TensorFlow Probability with TensorFlow, please install the tf-keras or tf-keras-nightly package.
This can be be done through installing the tensorflow-probability[tf] extra.
/Users/paulnitschke/Desktop/projects/geo_meta_rl/garage/experiment/deterministic.py:36: UserWarning: Enabeling deterministic mode in PyTorch can have a performance impact when using GPU.
  warnings.warn(


2025-01-08 09:31:28 | [CL_point_env] epoch #0 | Finished meta-testing...
2025-01-08 09:31:28 | [CL_point_env] epoch #0 | Sampling for adapation and meta-testing...


Failed to import TF-Keras. Please note that TF-Keras is not installed by default when you install TensorFlow Probability. This is so that JAX-only users do not have to install TensorFlow or TF-Keras. To use TensorFlow Probability with TensorFlow, please install the tf-keras or tf-keras-nightly package.
This can be be done through installing the tensorflow-probability[tf] extra.


2025-01-08 09:31:37 | [CL_point_env] epoch #0 | Finished meta-testing...
2025-01-08 09:31:37 | [CL_point_env] epoch #0 | Saving snapshot...
2025-01-08 09:31:37 | [CL_point_env] epoch #0 | Saved
2025-01-08 09:31:37 | [CL_point_env] epoch #0 | Time 326.28 s
2025-01-08 09:31:37 | [CL_point_env] epoch #0 | EpochTime 326.28 s
--------------------------------------------------  ------------
Embedding/MeanContrastiveLoss                          0.0578852
MetaTest/Average/AverageDiscountedReturn            -480.349
MetaTest/Average/AverageReturn                      -480.349
MetaTest/Average/Iteration                             0
MetaTest/Average/MaxReturn                          -286.006
MetaTest/Average/MinReturn                          -563.339
MetaTest/Average/NumEpisodes                          20
MetaTest/Average/StdReturn                            83.5783
MetaTest/Average/SuccessRate                           0
MetaTest/Average/TerminationRate                       0
MetaTest/__unnamed_task__/AverageDiscountedReturn   -480.349
MetaTest/__unnamed_task__/AverageReturn             -480.349
MetaTest/__unnamed_task__/Iteration                    0
MetaTest/__unnamed_task__/MaxReturn                 -286.006
MetaTest/__unnamed_task__/MinReturn                 -563.339
MetaTest/__unnamed_task__/NumEpisodes                 20
MetaTest/__unnamed_task__/StdReturn                   83.5783
MetaTest/__unnamed_task__/SuccessRate                  0
MetaTest/__unnamed_task__/TerminationRate              0
MetaTrain/Average/AverageDiscountedReturn           -557.496
MetaTrain/Average/AverageReturn                     -557.496
MetaTrain/Average/Iteration                            0
MetaTrain/Average/MaxReturn                         -551.652
MetaTrain/Average/MinReturn                         -563.339
MetaTrain/Average/NumEpisodes                          2
MetaTrain/Average/StdReturn                            5.84347
MetaTrain/Average/SuccessRate                          0
MetaTrain/Average/TerminationRate                      0
MetaTrain/__unnamed_task__/AverageDiscountedReturn  -557.496
MetaTrain/__unnamed_task__/AverageReturn            -557.496
MetaTrain/__unnamed_task__/Iteration                   0
MetaTrain/__unnamed_task__/MaxReturn                -551.652
MetaTrain/__unnamed_task__/MinReturn                -563.339
MetaTrain/__unnamed_task__/NumEpisodes                 2
MetaTrain/__unnamed_task__/StdReturn                   5.84347
MetaTrain/__unnamed_task__/SuccessRate                 0
MetaTrain/__unnamed_task__/TerminationRate             0
PolicyTraining/MeanQ1Vals                            -62.6757
PolicyTraining/MeanQ2Vals                            -62.6757
PolicyTraining/MeanVVals                             -62.2326
PolicyTraining/PolicyLoss                             62.2317
PolicyTraining/QfLoss                                  0.0677262
PolicyTraining/VfLoss                                  0.0717044
TotalEnvSteps                                       6900
--------------------------------------------------  ------------
2025-01-08 09:31:39 | [CL_point_env] epoch #1 | Training...
2025-01-08 09:31:43 | [CL_point_env] epoch #1 | Pairwise Similarity Matrix: tensor([[ 1.0000, -0.9910],
        [-0.9910,  1.0000]], grad_fn=<MmBackward0>)
2025-01-08 09:31:43 | [CL_point_env] epoch #1 | Training Encoder

-----------------------------  ----------
Embedding/MeanContrastiveLoss    0.226217
PolicyTraining/MeanQ1Vals      -79.6444
PolicyTraining/MeanQ2Vals      -79.6444
PolicyTraining/MeanVVals       -78.837
PolicyTraining/PolicyLoss       78.8873
PolicyTraining/QfLoss            0.992861
PolicyTraining/VfLoss            0.309821
-----------------------------  ----------
-----------------------------  ----------
Embedding/MeanContrastiveLoss    0.226217
PolicyTraining/MeanQ1Vals      -79.6444
PolicyTraining/MeanQ2Vals      -79.6444
PolicyTraining/MeanVVals       -78.837
PolicyTraining/PolicyLoss       78.8873
PolicyTraining/QfLoss            0.992861
PolicyTraining/VfLoss            0.309821
-----------------------------  ----------
2025-01-08 09:32:04 | [CL_point_env] epoch #1 | Pairwise Similarity Matrix: tensor([[ 1.0000, -0.9725],
        [-0.9725,  1.0000]], grad_fn=<MmBackward0>)
2025-01-08 09:32:04 | [CL_point_env] epoch #1 | Training Encoder

-----------------------------  ----------
Embedding/MeanContrastiveLoss    0.117733
PolicyTraining/MeanQ1Vals      -84.71
PolicyTraining/MeanQ2Vals      -84.71
PolicyTraining/MeanVVals       -84.007
PolicyTraining/PolicyLoss       84.0439
PolicyTraining/QfLoss            1.10192
PolicyTraining/VfLoss            0.158689
-----------------------------  ----------
-----------------------------  ----------
Embedding/MeanContrastiveLoss    0.117733
PolicyTraining/MeanQ1Vals      -84.71
PolicyTraining/MeanQ2Vals      -84.71
PolicyTraining/MeanVVals       -84.007
PolicyTraining/PolicyLoss       84.0439
PolicyTraining/QfLoss            1.10192
PolicyTraining/VfLoss            0.158689
-----------------------------  ----------
2025-01-08 09:32:26 | [CL_point_env] epoch #1 | Pairwise Similarity Matrix: tensor([[ 1.0000, -0.9988],
        [-0.9988,  1.0000]], grad_fn=<MmBackward0>)
2025-01-08 09:32:26 | [CL_point_env] epoch #1 | Training Encoder

-----------------------------  ----------
Embedding/MeanContrastiveLoss    0.135813
PolicyTraining/MeanQ1Vals      -88.6458
PolicyTraining/MeanQ2Vals      -88.6458
PolicyTraining/MeanVVals       -87.9817
PolicyTraining/PolicyLoss       88.0424
PolicyTraining/QfLoss            1.6106
PolicyTraining/VfLoss            0.166872
-----------------------------  ----------
-----------------------------  ----------
Embedding/MeanContrastiveLoss    0.135813
PolicyTraining/MeanQ1Vals      -88.6458
PolicyTraining/MeanQ2Vals      -88.6458
PolicyTraining/MeanVVals       -87.9817
PolicyTraining/PolicyLoss       88.0424
PolicyTraining/QfLoss            1.6106
PolicyTraining/VfLoss            0.166872
-----------------------------  ----------
2025-01-08 09:32:45 | [CL_point_env] epoch #1 | Pairwise Similarity Matrix: tensor([[ 1.0000, -0.9963],
        [-0.9963,  1.0000]], grad_fn=<MmBackward0>)
2025-01-08 09:32:45 | [CL_point_env] epoch #1 | Training Encoder

-----------------------------  -----------
Embedding/MeanContrastiveLoss    0.0579877
PolicyTraining/MeanQ1Vals      -94.1924
PolicyTraining/MeanQ2Vals      -94.1924
PolicyTraining/MeanVVals       -93.6821
PolicyTraining/PolicyLoss       93.6607
PolicyTraining/QfLoss            0.645092
PolicyTraining/VfLoss            0.140479
-----------------------------  -----------
-----------------------------  -----------
Embedding/MeanContrastiveLoss    0.0579877
PolicyTraining/MeanQ1Vals      -94.1924
PolicyTraining/MeanQ2Vals      -94.1924
PolicyTraining/MeanVVals       -93.6821
PolicyTraining/PolicyLoss       93.6607
PolicyTraining/QfLoss            0.645092
PolicyTraining/VfLoss            0.140479
-----------------------------  -----------
2025-01-08 09:33:06 | [CL_point_env] epoch #1 | Pairwise Similarity Matrix: tensor([[ 1.0000, -0.9957],
        [-0.9957,  1.0000]], grad_fn=<MmBackward0>)
2025-01-08 09:33:06 | [CL_point_env] epoch #1 | Training Encoder

-----------------------------  -----------
Embedding/MeanContrastiveLoss    0.0580483
PolicyTraining/MeanQ1Vals      -97.008
PolicyTraining/MeanQ2Vals      -97.008
PolicyTraining/MeanVVals       -96.4804
PolicyTraining/PolicyLoss       96.5384
PolicyTraining/QfLoss            0.605204
PolicyTraining/VfLoss            0.143258
-----------------------------  -----------
-----------------------------  -----------
Embedding/MeanContrastiveLoss    0.0580483
PolicyTraining/MeanQ1Vals      -97.008
PolicyTraining/MeanQ2Vals      -97.008
PolicyTraining/MeanVVals       -96.4804
PolicyTraining/PolicyLoss       96.5384
PolicyTraining/QfLoss            0.605204
PolicyTraining/VfLoss            0.143258
-----------------------------  -----------
2025-01-08 09:33:26 | [CL_point_env] epoch #1 | Pairwise Similarity Matrix: tensor([[ 1.0000, -0.9978],
        [-0.9978,  1.0000]], grad_fn=<MmBackward0>)
2025-01-08 09:33:26 | [CL_point_env] epoch #1 | Training Encoder
Traceback (most recent call last):
  File "/Users/paulnitschke/Desktop/projects/geo_meta_rl/garage/examples/torch/multi_env_CL.py", line 199, in <module>
    CL_point_env()
  File "/Users/paulnitschke/Desktop/projects/geo_meta_rl/garage/experiment/experiment.py", line 369, in __call__
    result = self.function(ctxt, **kwargs)
  File "/Users/paulnitschke/Desktop/projects/geo_meta_rl/garage/examples/torch/multi_env_CL.py", line 196, in CL_point_env
    trainer.train(n_epochs=num_epochs, batch_size=batch_size)
  File "/Users/paulnitschke/Desktop/projects/geo_meta_rl/garage/trainer.py", line 396, in train
    average_return = self._algo.train(self)
  File "/Users/paulnitschke/Desktop/projects/geo_meta_rl/garage/torch/algos/CLMeta.py", line 335, in train
    self._train_once()
  File "/Users/paulnitschke/Desktop/projects/geo_meta_rl/garage/torch/algos/CLMeta.py", line 373, in _train_once
    self._optimize_policy(indices)
  File "/Users/paulnitschke/Desktop/projects/geo_meta_rl/garage/torch/algos/CLMeta.py", line 493, in _optimize_policy
    (q2_pred - q_target)**2)
  File "/Users/paulnitschke/miniconda3/envs/env_geo_meta_rl/lib/python3.9/site-packages/torch/_tensor.py", line 525, in backward
    torch.autograd.backward(
  File "/Users/paulnitschke/miniconda3/envs/env_geo_meta_rl/lib/python3.9/site-packages/torch/autograd/__init__.py", line 267, in backward
    _engine_run_backward(
  File "/Users/paulnitschke/miniconda3/envs/env_geo_meta_rl/lib/python3.9/site-packages/torch/autograd/graph.py", line 744, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
KeyboardInterrupt
Traceback (most recent call last):
  File "/Users/paulnitschke/Desktop/projects/geo_meta_rl/garage/examples/torch/multi_env_CL.py", line 199, in <module>
    CL_point_env()
  File "/Users/paulnitschke/Desktop/projects/geo_meta_rl/garage/experiment/experiment.py", line 369, in __call__
    result = self.function(ctxt, **kwargs)
  File "/Users/paulnitschke/Desktop/projects/geo_meta_rl/garage/examples/torch/multi_env_CL.py", line 196, in CL_point_env
    trainer.train(n_epochs=num_epochs, batch_size=batch_size)
  File "/Users/paulnitschke/Desktop/projects/geo_meta_rl/garage/trainer.py", line 396, in train
    average_return = self._algo.train(self)
  File "/Users/paulnitschke/Desktop/projects/geo_meta_rl/garage/torch/algos/CLMeta.py", line 335, in train
    self._train_once()
  File "/Users/paulnitschke/Desktop/projects/geo_meta_rl/garage/torch/algos/CLMeta.py", line 373, in _train_once
    self._optimize_policy(indices)
  File "/Users/paulnitschke/Desktop/projects/geo_meta_rl/garage/torch/algos/CLMeta.py", line 493, in _optimize_policy
    (q2_pred - q_target)**2)
  File "/Users/paulnitschke/miniconda3/envs/env_geo_meta_rl/lib/python3.9/site-packages/torch/_tensor.py", line 525, in backward
    torch.autograd.backward(
  File "/Users/paulnitschke/miniconda3/envs/env_geo_meta_rl/lib/python3.9/site-packages/torch/autograd/__init__.py", line 267, in backward
    _engine_run_backward(
  File "/Users/paulnitschke/miniconda3/envs/env_geo_meta_rl/lib/python3.9/site-packages/torch/autograd/graph.py", line 744, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
KeyboardInterrupt
